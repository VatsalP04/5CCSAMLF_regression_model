{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80230bf0",
   "metadata": {},
   "source": [
    "# Feature Engineering and Preprocessing\n",
    "This notebook covers the feature engineering steps applied to the dataset, including encoding categorical variables, feature scaling, dimensionality reduction, and outlier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7339cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 31 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   outcome  10000 non-null  float64\n",
      " 1   carat    10000 non-null  float64\n",
      " 2   cut      10000 non-null  object \n",
      " 3   color    10000 non-null  object \n",
      " 4   clarity  10000 non-null  object \n",
      " 5   depth    10000 non-null  float64\n",
      " 6   table    10000 non-null  float64\n",
      " 7   price    10000 non-null  int64  \n",
      " 8   x        10000 non-null  float64\n",
      " 9   y        10000 non-null  float64\n",
      " 10  z        10000 non-null  float64\n",
      " 11  a1       10000 non-null  float64\n",
      " 12  a2       10000 non-null  float64\n",
      " 13  a3       10000 non-null  float64\n",
      " 14  a4       10000 non-null  float64\n",
      " 15  a5       10000 non-null  float64\n",
      " 16  b1       10000 non-null  float64\n",
      " 17  b2       10000 non-null  float64\n",
      " 18  b3       10000 non-null  float64\n",
      " 19  b4       10000 non-null  float64\n",
      " 20  b5       10000 non-null  float64\n",
      " 21  a6       10000 non-null  float64\n",
      " 22  a7       10000 non-null  float64\n",
      " 23  a8       10000 non-null  float64\n",
      " 24  a9       10000 non-null  float64\n",
      " 25  a10      10000 non-null  float64\n",
      " 26  b6       10000 non-null  float64\n",
      " 27  b7       10000 non-null  float64\n",
      " 28  b8       10000 non-null  float64\n",
      " 29  b9       10000 non-null  float64\n",
      " 30  b10      10000 non-null  float64\n",
      "dtypes: float64(27), int64(1), object(3)\n",
      "memory usage: 2.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "      outcome  carat        cut color clarity  depth  table  price     x     y  \\\n",
       " 0 -26.701232   1.14      Ideal     G     VS1   62.3   56.0   7948  6.73  6.70   \n",
       " 1   6.548093   0.38    Premium     H     VS2   60.5   59.0    898  4.69  4.66   \n",
       " 2   6.612562   0.50  Very Good     E     SI1   60.7   58.0   1351  5.09  5.13   \n",
       " 3  -5.073562   0.70    Premium     D     SI1   61.2   58.0   2512  5.74  5.70   \n",
       " 4 -14.436557   0.83      Ideal     G     SI2   62.4   54.0   2751  6.01  6.08   \n",
       " \n",
       "       z        a1        a2        a3        a4        a5        b1        b2  \\\n",
       " 0  4.18  0.709585  0.385796  0.267058  0.500222  0.462145  0.806922  0.891912   \n",
       " 1  2.83  0.649532  0.037578  0.618528  0.052079  0.008600  0.496153  0.092878   \n",
       " 2  3.10  0.550496  0.029469  0.350697  0.131802  0.317685  0.821415  0.581294   \n",
       " 3  3.50  0.982447  0.487176  0.339991  0.232601  0.267207  0.800913  0.984788   \n",
       " 4  3.77  0.030877  0.818540  0.955872  0.923147  0.861377  0.997349  0.091662   \n",
       " \n",
       "          b3        b4        b5        a6        a7        a8        a9  \\\n",
       " 0  0.276683  0.967510  0.131087  0.168836 -0.273758  1.107832  1.247795   \n",
       " 1  0.089039  0.321334  0.549223 -0.256549  0.315373 -0.030326 -0.114335   \n",
       " 2  0.876056  0.743479  0.916367 -1.193327 -0.657307 -0.591726 -0.446856   \n",
       " 3  0.070506  0.528945  0.005256 -1.740788 -1.778860 -0.825070  0.444932   \n",
       " 4  0.781069  0.019354  0.522191 -0.859322  1.409268  0.861992  1.109063   \n",
       " \n",
       "         a10        b6        b7        b8        b9       b10  \n",
       " 0  0.482344  0.489511 -0.321138  0.573382  0.446871 -1.990581  \n",
       " 1 -1.059588 -1.761360 -1.343951 -1.002550 -0.225030 -0.446653  \n",
       " 2 -0.765286 -0.816544 -1.397794 -0.477130  0.810509  1.725131  \n",
       " 3  1.173109  0.453606 -0.263440  0.246210 -0.850503 -0.412950  \n",
       " 4 -1.436722 -1.461618  0.081787  0.258087  0.851146  2.204813  )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"CW1_train.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information\n",
    "df.info(), df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe95085",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913f7ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers detected at positions: (array([  13,   19,   19,   23,   35,   38,   56,   61,  106,  118,  118,\n",
      "        124,  138,  146,  175,  189,  221,  221,  222,  227,  227,  247,\n",
      "        254,  261,  271,  284,  290,  308,  308,  309,  320,  323,  348,\n",
      "        361,  383,  397,  416,  424,  448,  448,  481,  486,  496,  504,\n",
      "        504,  521,  522,  535,  538,  576,  577,  582,  651,  655,  657,\n",
      "        661,  661,  661,  670,  672,  683,  717,  724,  726,  735,  736,\n",
      "        786,  808,  817,  829,  835,  835,  840,  889,  900,  900,  904,\n",
      "        933,  970,  979,  989,  994, 1020, 1022, 1022, 1029, 1056, 1068,\n",
      "       1088, 1090, 1092, 1116, 1130, 1146, 1158, 1166, 1168, 1182, 1217,\n",
      "       1247, 1282, 1282, 1290, 1306, 1312, 1319, 1329, 1333, 1350, 1360,\n",
      "       1360, 1360, 1364, 1364, 1369, 1389, 1421, 1429, 1465, 1489, 1491,\n",
      "       1492, 1514, 1525, 1541, 1551, 1559, 1571, 1575, 1575, 1584, 1589,\n",
      "       1605, 1607, 1620, 1623, 1625, 1643, 1660, 1666, 1681, 1693, 1702,\n",
      "       1703, 1715, 1779, 1795, 1828, 1834, 1834, 1864, 1865, 1885, 1896,\n",
      "       1933, 1934, 1972, 1975, 1981, 2012, 2012, 2025, 2025, 2028, 2061,\n",
      "       2082, 2082, 2098, 2109, 2128, 2134, 2134, 2146, 2156, 2168, 2178,\n",
      "       2216, 2223, 2243, 2246, 2248, 2251, 2258, 2266, 2302, 2307, 2310,\n",
      "       2312, 2330, 2330, 2333, 2344, 2367, 2416, 2472, 2477, 2495, 2513,\n",
      "       2528, 2528, 2533, 2539, 2551, 2553, 2590, 2592, 2627, 2627, 2628,\n",
      "       2634, 2642, 2654, 2654, 2670, 2686, 2690, 2705, 2705, 2712, 2722,\n",
      "       2722, 2722, 2726, 2732, 2755, 2775, 2775, 2778, 2794, 2805, 2827,\n",
      "       2844, 2850, 2866, 2882, 2929, 2939, 2945, 2984, 3017, 3017, 3022,\n",
      "       3041, 3063, 3063, 3065, 3092, 3109, 3112, 3147, 3149, 3164, 3168,\n",
      "       3171, 3187, 3195, 3195, 3198, 3217, 3227, 3240, 3244, 3254, 3255,\n",
      "       3288, 3291, 3291, 3294, 3300, 3318, 3330, 3330, 3345, 3346, 3367,\n",
      "       3379, 3408, 3408, 3409, 3427, 3433, 3455, 3462, 3467, 3481, 3483,\n",
      "       3483, 3512, 3542, 3560, 3590, 3634, 3665, 3680, 3680, 3683, 3683,\n",
      "       3713, 3717, 3728, 3738, 3749, 3749, 3764, 3764, 3765, 3769, 3814,\n",
      "       3815, 3820, 3826, 3826, 3836, 3844, 3857, 3911, 3912, 3979, 4011,\n",
      "       4017, 4037, 4039, 4040, 4058, 4066, 4084, 4097, 4097, 4101, 4101,\n",
      "       4111, 4117, 4147, 4150, 4155, 4162, 4177, 4192, 4194, 4194, 4213,\n",
      "       4230, 4249, 4254, 4261, 4263, 4289, 4299, 4307, 4307, 4309, 4309,\n",
      "       4322, 4355, 4358, 4360, 4360, 4360, 4360, 4360, 4364, 4376, 4403,\n",
      "       4419, 4426, 4426, 4432, 4432, 4432, 4432, 4432, 4434, 4459, 4476,\n",
      "       4481, 4504, 4538, 4542, 4546, 4583, 4593, 4593, 4602, 4617, 4628,\n",
      "       4634, 4638, 4662, 4679, 4699, 4702, 4707, 4728, 4729, 4731, 4734,\n",
      "       4735, 4755, 4757, 4788, 4797, 4797, 4841, 4877, 4881, 4891, 4892,\n",
      "       4892, 4902, 4911, 4917, 4924, 4982, 4989, 5000, 5000, 5000, 5000,\n",
      "       5002, 5005, 5039, 5056, 5061, 5068, 5075, 5101, 5108, 5109, 5122,\n",
      "       5135, 5171, 5181, 5185, 5189, 5189, 5192, 5192, 5193, 5193, 5194,\n",
      "       5194, 5194, 5200, 5237, 5241, 5248, 5292, 5296, 5341, 5359, 5401,\n",
      "       5401, 5410, 5425, 5435, 5435, 5441, 5466, 5479, 5521, 5530, 5575,\n",
      "       5577, 5600, 5611, 5617, 5619, 5619, 5663, 5671, 5671, 5694, 5694,\n",
      "       5694, 5703, 5705, 5706, 5707, 5707, 5720, 5727, 5741, 5741, 5755,\n",
      "       5757, 5757, 5757, 5767, 5770, 5776, 5777, 5781, 5786, 5787, 5846,\n",
      "       5855, 5855, 5861, 5869, 5869, 5869, 5869, 5869, 5873, 5876, 5877,\n",
      "       5877, 5906, 5916, 5920, 5937, 5940, 5965, 5989, 6017, 6057, 6057,\n",
      "       6075, 6094, 6098, 6111, 6117, 6118, 6135, 6143, 6143, 6143, 6151,\n",
      "       6177, 6191, 6203, 6223, 6244, 6253, 6253, 6254, 6287, 6297, 6298,\n",
      "       6354, 6399, 6426, 6475, 6475, 6478, 6479, 6489, 6498, 6499, 6525,\n",
      "       6527, 6541, 6552, 6572, 6579, 6604, 6604, 6604, 6626, 6630, 6630,\n",
      "       6644, 6658, 6658, 6667, 6672, 6689, 6704, 6704, 6722, 6722, 6732,\n",
      "       6736, 6752, 6759, 6767, 6774, 6777, 6788, 6803, 6804, 6804, 6857,\n",
      "       6919, 6924, 6928, 6968, 6980, 6989, 7032, 7046, 7086, 7091, 7096,\n",
      "       7096, 7104, 7112, 7117, 7148, 7148, 7165, 7169, 7171, 7171, 7183,\n",
      "       7202, 7219, 7231, 7234, 7289, 7289, 7319, 7320, 7376, 7379, 7391,\n",
      "       7403, 7414, 7421, 7474, 7483, 7485, 7487, 7499, 7499, 7501, 7501,\n",
      "       7507, 7514, 7542, 7580, 7580, 7581, 7584, 7594, 7598, 7598, 7605,\n",
      "       7607, 7624, 7624, 7641, 7647, 7647, 7664, 7665, 7672, 7692, 7695,\n",
      "       7730, 7766, 7783, 7783, 7783, 7797, 7801, 7804, 7811, 7821, 7847,\n",
      "       7855, 7858, 7865, 7871, 7873, 7873, 7884, 7914, 7916, 7949, 7955,\n",
      "       7965, 7965, 7967, 7975, 7986, 7996, 8009, 8022, 8029, 8055, 8063,\n",
      "       8099, 8127, 8154, 8163, 8166, 8180, 8187, 8187, 8198, 8219, 8236,\n",
      "       8248, 8256, 8257, 8258, 8273, 8297, 8315, 8324, 8330, 8330, 8334,\n",
      "       8334, 8336, 8336, 8336, 8336, 8336, 8336, 8342, 8349, 8375, 8379,\n",
      "       8379, 8381, 8383, 8385, 8399, 8407, 8416, 8427, 8435, 8445, 8455,\n",
      "       8463, 8476, 8484, 8501, 8516, 8516, 8547, 8551, 8558, 8569, 8581,\n",
      "       8588, 8593, 8603, 8618, 8625, 8633, 8633, 8639, 8646, 8646, 8677,\n",
      "       8680, 8689, 8729, 8729, 8735, 8750, 8756, 8756, 8756, 8756, 8777,\n",
      "       8779, 8782, 8784, 8793, 8802, 8850, 8904, 8915, 8923, 8932, 8955,\n",
      "       8990, 8992, 9003, 9025, 9025, 9030, 9030, 9050, 9055, 9064, 9071,\n",
      "       9072, 9085, 9100, 9112, 9113, 9122, 9166, 9166, 9167, 9168, 9212,\n",
      "       9245, 9245, 9263, 9267, 9267, 9269, 9272, 9272, 9283, 9310, 9310,\n",
      "       9334, 9345, 9350, 9351, 9365, 9373, 9375, 9382, 9426, 9458, 9463,\n",
      "       9468, 9471, 9471, 9494, 9520, 9524, 9533, 9535, 9537, 9569, 9574,\n",
      "       9597, 9610, 9636, 9653, 9655, 9686, 9690, 9698, 9714, 9714, 9743,\n",
      "       9776, 9776, 9787, 9790, 9803, 9810, 9811, 9813, 9815, 9826, 9834,\n",
      "       9861, 9899, 9924, 9929, 9959, 9969, 9977, 9990, 9996]), array([ 4,  1,  4, 25, 20,  1,  4, 23,  2,  1,  4,  3, 25, 22,  2, 25,  1,\n",
      "        4, 18,  2,  3, 26,  4,  2,  4,  2,  2,  1,  4,  4,  3,  2,  1,  3,\n",
      "        4, 25, 21,  2,  2,  3, 23,  2,  4, 18, 19,  2, 19, 23,  2, 25,  4,\n",
      "        4, 25,  4,  4,  1,  5,  7, 27,  4, 18,  4,  4,  2,  4, 21,  4,  1,\n",
      "       26,  4,  1,  4, 27,  4, 19, 24,  3,  4, 23, 27, 19,  3,  0,  2,  4,\n",
      "        2, 27,  4,  2, 24,  2, 22, 22,  1,  4, 24,  4,  4,  4,  4,  1,  4,\n",
      "        1, 26,  1,  4,  4, 26,  3, 23, 24, 25,  1,  4,  4,  2,  4,  0, 21,\n",
      "       22,  3,  4,  4, 24, 20, 24,  2,  1, 25, 26,  4,  3,  2,  2,  4,  2,\n",
      "       26,  4, 19,  1,  3,  4, 25,  4,  4,  4,  3, 19,  1,  4, 27,  4,  3,\n",
      "       25,  4,  4,  4, 18,  0,  1,  3,  1,  4,  4,  4,  2, 25,  2, 25,  3,\n",
      "       25, 26,  3,  4,  3,  4, 19, 21,  2,  4,  4,  3, 23,  2, 21,  4, 20,\n",
      "        1, 22, 23,  2,  4,  4,  4, 23,  3,  3,  0,  1,  4, 20, 20,  4, 18,\n",
      "       23,  4,  4, 20,  4,  7,  2,  1,  4, 22, 24,  2, 25, 26,  4,  5,  6,\n",
      "        7,  4, 27,  1, 19, 20, 20, 25, 19, 18,  2,  2,  3, 24,  4, 24,  1,\n",
      "        4,  2,  3,  4, 26,  1,  4,  2,  2,  2,  4,  4, 23, 20, 27,  3, 19,\n",
      "       20, 21,  4,  4, 19,  4, 23,  4,  1,  4,  1,  4,  4, 24,  4,  1,  4,\n",
      "        3, 18,  2,  3,  1,  2,  3,  1, 21, 27,  4, 24,  2,  2,  3,  2,  1,\n",
      "        4,  3,  2, 26,  1,  4, 20, 21,  4,  4,  2,  4,  1,  4,  2,  3, 24,\n",
      "       23,  2, 25,  4,  1,  4,  4, 22, 23,  2, 24, 18, 20, 26, 21, 19, 23,\n",
      "       20, 27, 18,  2,  3,  1,  4,  2,  3,  3, 24, 19, 25,  2,  4,  1,  4,\n",
      "        4, 18, 22,  4,  3,  4,  1,  1,  1,  2,  1,  4,  3,  4, 27,  1,  2,\n",
      "        4,  5,  7,  2, 26,  4, 24,  2,  3,  1,  4,  5,  6,  7,  4, 24, 18,\n",
      "        2,  4, 24, 25, 19, 23,  2,  3, 25,  3,  2,  4, 25, 24, 23,  2, 21,\n",
      "        4, 21,  7,  2,  3,  4,  4, 23,  4,  1,  4,  4,  2,  4,  2,  1,  4,\n",
      "        4,  4,  2,  1, 21, 21,  1,  4,  5,  7,  4, 26,  4, 21,  4, 26, 21,\n",
      "        3,  4,  4, 18, 26,  4,  4,  4,  1,  4,  1,  4, 23, 24,  1,  5,  7,\n",
      "        4,  1, 22,  2, 20,  4,  4,  2,  1,  4, 23, 23,  2,  3, 22, 25,  2,\n",
      "       22, 22, 20,  0,  3,  4,  4,  1,  4,  4,  1,  4,  1,  2,  3, 20, 18,\n",
      "       22,  2,  3,  2, 21, 19, 20,  2,  1,  4,  5, 20, 24, 27, 23,  4, 26,\n",
      "       27,  4,  1,  4, 27,  1,  4,  5,  6,  7, 21,  2,  1,  2, 24,  2, 21,\n",
      "        2,  4,  2,  1,  3, 19, 20, 23, 23,  2,  0, 27,  4,  4,  1,  2,  3,\n",
      "        4,  4, 19,  4, 20, 21,  4, 25, 23, 19,  2,  4,  4,  3, 26,  1,  4,\n",
      "        4, 25,  4, 22, 22,  2,  4, 25,  2, 25,  3,  1,  4,  7, 19,  1,  4,\n",
      "       26,  1,  4, 20,  3, 23,  1,  5,  2,  3,  4,  3, 22, 25, 23,  3, 27,\n",
      "       26,  2,  2,  3,  3, 21,  1, 26,  2,  2,  2,  3,  3,  2,  4,  2, 25,\n",
      "       24,  4,  2,  2,  3,  4, 18,  2,  3, 20, 18, 22, 27,  4, 22, 24, 25,\n",
      "       21,  4, 23,  4,  2,  2, 18,  1,  4,  4, 22, 18, 19,  2,  3,  0, 18,\n",
      "        2,  1,  4,  4,  4, 19,  1,  4, 24,  0,  1,  4,  4,  3, 19, 23,  2,\n",
      "        4,  4,  4,  1, 22,  1,  4,  7,  2,  3, 27,  4,  4, 21,  4,  4,  3,\n",
      "        2,  1,  4,  2, 21,  3,  4, 19,  1,  4, 18,  4,  2, 18,  1,  3,  4,\n",
      "        4,  2,  4, 18,  1,  3, 26,  4,  1,  4, 25, 21,  2,  4,  4,  4, 21,\n",
      "       26, 19,  4,  0,  1,  4, 26, 27,  1,  4,  5,  6,  7, 21,  4,  4,  1,\n",
      "        2,  3,  4,  4, 18,  3,  4,  4,  2,  4,  2,  1, 27,  4, 21,  4,  3,\n",
      "        4,  4,  1,  2,  2,  0, 26, 19, 20, 20,  4,  2,  3,  4,  1,  4,  2,\n",
      "        3, 26,  1,  4,  3,  1,  1,  4,  5,  7, 25, 20, 24, 25,  4,  3,  3,\n",
      "       21, 26, 25, 25,  4,  2,  3,  2,  1,  4,  2,  3,  2, 23, 27, 20, 18,\n",
      "        4, 26, 23,  2,  3, 20, 21, 22, 20,  1,  1,  4, 22,  2,  3,  4,  2,\n",
      "        3, 18,  1,  4,  4,  2, 26, 19, 23, 22, 21,  2, 27,  4, 20,  1,  6,\n",
      "        7,  2, 27, 22,  2,  4,  2,  1,  3,  2,  2,  4, 22,  4, 18,  2, 18,\n",
      "        1,  2, 27,  1,  4,  2,  0,  4,  4, 23,  4, 23, 27,  1, 27, 19, 20,\n",
      "        0, 27,  3,  2, 25,  2]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate Z-scores\n",
    "z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "\n",
    "# Set a threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# Identify outliers\n",
    "outliers = np.where(z_scores > threshold)\n",
    "\n",
    "# Display outliers\n",
    "print(\"Outliers detected at positions:\", outliers)\n",
    "\n",
    "# Visualize outliers using a boxplot\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(data=df.select_dtypes(include=[np.number]))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Boxplot to visualize outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e335d2",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ce93a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd4373b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>a6</th>\n",
       "      <th>a7</th>\n",
       "      <th>a8</th>\n",
       "      <th>a9</th>\n",
       "      <th>a10</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>cut_Good</th>\n",
       "      <th>cut_Ideal</th>\n",
       "      <th>cut_Premium</th>\n",
       "      <th>cut_Very Good</th>\n",
       "      <th>color_E</th>\n",
       "      <th>color_F</th>\n",
       "      <th>color_G</th>\n",
       "      <th>color_H</th>\n",
       "      <th>color_I</th>\n",
       "      <th>color_J</th>\n",
       "      <th>clarity_IF</th>\n",
       "      <th>clarity_SI1</th>\n",
       "      <th>clarity_SI2</th>\n",
       "      <th>clarity_VS1</th>\n",
       "      <th>clarity_VS2</th>\n",
       "      <th>clarity_VVS1</th>\n",
       "      <th>clarity_VVS2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-26.701232</td>\n",
       "      <td>1.14</td>\n",
       "      <td>62.3</td>\n",
       "      <td>56.0</td>\n",
       "      <td>7948</td>\n",
       "      <td>6.73</td>\n",
       "      <td>6.70</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.709585</td>\n",
       "      <td>0.385796</td>\n",
       "      <td>0.267058</td>\n",
       "      <td>0.500222</td>\n",
       "      <td>0.462145</td>\n",
       "      <td>0.806922</td>\n",
       "      <td>0.891912</td>\n",
       "      <td>0.276683</td>\n",
       "      <td>0.967510</td>\n",
       "      <td>0.131087</td>\n",
       "      <td>0.168836</td>\n",
       "      <td>-0.273758</td>\n",
       "      <td>1.107832</td>\n",
       "      <td>1.247795</td>\n",
       "      <td>0.482344</td>\n",
       "      <td>0.489511</td>\n",
       "      <td>-0.321138</td>\n",
       "      <td>0.573382</td>\n",
       "      <td>0.446871</td>\n",
       "      <td>-1.990581</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.548093</td>\n",
       "      <td>0.38</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>898</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4.66</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.649532</td>\n",
       "      <td>0.037578</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.052079</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.496153</td>\n",
       "      <td>0.092878</td>\n",
       "      <td>0.089039</td>\n",
       "      <td>0.321334</td>\n",
       "      <td>0.549223</td>\n",
       "      <td>-0.256549</td>\n",
       "      <td>0.315373</td>\n",
       "      <td>-0.030326</td>\n",
       "      <td>-0.114335</td>\n",
       "      <td>-1.059588</td>\n",
       "      <td>-1.761360</td>\n",
       "      <td>-1.343951</td>\n",
       "      <td>-1.002550</td>\n",
       "      <td>-0.225030</td>\n",
       "      <td>-0.446653</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.612562</td>\n",
       "      <td>0.50</td>\n",
       "      <td>60.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1351</td>\n",
       "      <td>5.09</td>\n",
       "      <td>5.13</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.550496</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.350697</td>\n",
       "      <td>0.131802</td>\n",
       "      <td>0.317685</td>\n",
       "      <td>0.821415</td>\n",
       "      <td>0.581294</td>\n",
       "      <td>0.876056</td>\n",
       "      <td>0.743479</td>\n",
       "      <td>0.916367</td>\n",
       "      <td>-1.193327</td>\n",
       "      <td>-0.657307</td>\n",
       "      <td>-0.591726</td>\n",
       "      <td>-0.446856</td>\n",
       "      <td>-0.765286</td>\n",
       "      <td>-0.816544</td>\n",
       "      <td>-1.397794</td>\n",
       "      <td>-0.477130</td>\n",
       "      <td>0.810509</td>\n",
       "      <td>1.725131</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.073562</td>\n",
       "      <td>0.70</td>\n",
       "      <td>61.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2512</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.70</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.982447</td>\n",
       "      <td>0.487176</td>\n",
       "      <td>0.339991</td>\n",
       "      <td>0.232601</td>\n",
       "      <td>0.267207</td>\n",
       "      <td>0.800913</td>\n",
       "      <td>0.984788</td>\n",
       "      <td>0.070506</td>\n",
       "      <td>0.528945</td>\n",
       "      <td>0.005256</td>\n",
       "      <td>-1.740788</td>\n",
       "      <td>-1.778860</td>\n",
       "      <td>-0.825070</td>\n",
       "      <td>0.444932</td>\n",
       "      <td>1.173109</td>\n",
       "      <td>0.453606</td>\n",
       "      <td>-0.263440</td>\n",
       "      <td>0.246210</td>\n",
       "      <td>-0.850503</td>\n",
       "      <td>-0.412950</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.436557</td>\n",
       "      <td>0.83</td>\n",
       "      <td>62.4</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2751</td>\n",
       "      <td>6.01</td>\n",
       "      <td>6.08</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.030877</td>\n",
       "      <td>0.818540</td>\n",
       "      <td>0.955872</td>\n",
       "      <td>0.923147</td>\n",
       "      <td>0.861377</td>\n",
       "      <td>0.997349</td>\n",
       "      <td>0.091662</td>\n",
       "      <td>0.781069</td>\n",
       "      <td>0.019354</td>\n",
       "      <td>0.522191</td>\n",
       "      <td>-0.859322</td>\n",
       "      <td>1.409268</td>\n",
       "      <td>0.861992</td>\n",
       "      <td>1.109063</td>\n",
       "      <td>-1.436722</td>\n",
       "      <td>-1.461618</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.258087</td>\n",
       "      <td>0.851146</td>\n",
       "      <td>2.204813</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     outcome  carat  depth  table  price     x     y     z        a1  \\\n",
       "0 -26.701232   1.14   62.3   56.0   7948  6.73  6.70  4.18  0.709585   \n",
       "1   6.548093   0.38   60.5   59.0    898  4.69  4.66  2.83  0.649532   \n",
       "2   6.612562   0.50   60.7   58.0   1351  5.09  5.13  3.10  0.550496   \n",
       "3  -5.073562   0.70   61.2   58.0   2512  5.74  5.70  3.50  0.982447   \n",
       "4 -14.436557   0.83   62.4   54.0   2751  6.01  6.08  3.77  0.030877   \n",
       "\n",
       "         a2        a3        a4        a5        b1        b2        b3  \\\n",
       "0  0.385796  0.267058  0.500222  0.462145  0.806922  0.891912  0.276683   \n",
       "1  0.037578  0.618528  0.052079  0.008600  0.496153  0.092878  0.089039   \n",
       "2  0.029469  0.350697  0.131802  0.317685  0.821415  0.581294  0.876056   \n",
       "3  0.487176  0.339991  0.232601  0.267207  0.800913  0.984788  0.070506   \n",
       "4  0.818540  0.955872  0.923147  0.861377  0.997349  0.091662  0.781069   \n",
       "\n",
       "         b4        b5        a6        a7        a8        a9       a10  \\\n",
       "0  0.967510  0.131087  0.168836 -0.273758  1.107832  1.247795  0.482344   \n",
       "1  0.321334  0.549223 -0.256549  0.315373 -0.030326 -0.114335 -1.059588   \n",
       "2  0.743479  0.916367 -1.193327 -0.657307 -0.591726 -0.446856 -0.765286   \n",
       "3  0.528945  0.005256 -1.740788 -1.778860 -0.825070  0.444932  1.173109   \n",
       "4  0.019354  0.522191 -0.859322  1.409268  0.861992  1.109063 -1.436722   \n",
       "\n",
       "         b6        b7        b8        b9       b10  cut_Good  cut_Ideal  \\\n",
       "0  0.489511 -0.321138  0.573382  0.446871 -1.990581     False       True   \n",
       "1 -1.761360 -1.343951 -1.002550 -0.225030 -0.446653     False      False   \n",
       "2 -0.816544 -1.397794 -0.477130  0.810509  1.725131     False      False   \n",
       "3  0.453606 -0.263440  0.246210 -0.850503 -0.412950     False      False   \n",
       "4 -1.461618  0.081787  0.258087  0.851146  2.204813     False       True   \n",
       "\n",
       "   cut_Premium  cut_Very Good  color_E  color_F  color_G  color_H  color_I  \\\n",
       "0        False          False    False    False     True    False    False   \n",
       "1         True          False    False    False    False     True    False   \n",
       "2        False           True     True    False    False    False    False   \n",
       "3         True          False    False    False    False    False    False   \n",
       "4        False          False    False    False     True    False    False   \n",
       "\n",
       "   color_J  clarity_IF  clarity_SI1  clarity_SI2  clarity_VS1  clarity_VS2  \\\n",
       "0    False       False        False        False         True        False   \n",
       "1    False       False        False        False        False         True   \n",
       "2    False       False         True        False        False        False   \n",
       "3    False       False         True        False        False        False   \n",
       "4    False       False        False         True        False        False   \n",
       "\n",
       "   clarity_VVS1  clarity_VVS2  \n",
       "0         False         False  \n",
       "1         False         False  \n",
       "2         False         False  \n",
       "3         False         False  \n",
       "4         False         False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded = df.copy()\n",
    "# Encoding categorical features\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Display encoded dataset\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23db0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define feature set and target variable\n",
    "X_full = df_encoded.drop(columns=['outcome'])\n",
    "y = df['outcome']\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=50, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(objective=\"reg:squarederror\", random_state=42),\n",
    "    \"Linear Regression\": LinearRegression()\n",
    "}\n",
    "\n",
    "# # Perform cross-validation on all models\n",
    "# for name, model in models.items():\n",
    "#     score = cross_val_score(model, X_full, y, cv=5, scoring='r2').mean()\n",
    "#     print(f\"{name} R Score (All Features): {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3944d1",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa94f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Creating new features\n",
    "df_encoded['volume'] = df_encoded['x'] * df_encoded['y'] * df_encoded['z']\n",
    "\n",
    "df_encoded.columns\n",
    "\n",
    "corr_matrix = df_encoded[['x', 'y', 'z', 'volume']].corr()\n",
    "\n",
    "corr_matrix\n",
    "\n",
    "df_encoded.drop(columns=['x', 'y', 'z'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eee0c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>a6</th>\n",
       "      <th>a7</th>\n",
       "      <th>a8</th>\n",
       "      <th>a9</th>\n",
       "      <th>a10</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>cut_Good</th>\n",
       "      <th>cut_Ideal</th>\n",
       "      <th>cut_Premium</th>\n",
       "      <th>cut_Very Good</th>\n",
       "      <th>color_E</th>\n",
       "      <th>color_F</th>\n",
       "      <th>color_G</th>\n",
       "      <th>color_H</th>\n",
       "      <th>color_I</th>\n",
       "      <th>color_J</th>\n",
       "      <th>clarity_IF</th>\n",
       "      <th>clarity_SI1</th>\n",
       "      <th>clarity_SI2</th>\n",
       "      <th>clarity_VS1</th>\n",
       "      <th>clarity_VS2</th>\n",
       "      <th>clarity_VVS1</th>\n",
       "      <th>clarity_VVS2</th>\n",
       "      <th>volume</th>\n",
       "      <th>log_price</th>\n",
       "      <th>log_carat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-26.701232</td>\n",
       "      <td>62.3</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.709585</td>\n",
       "      <td>0.385796</td>\n",
       "      <td>0.267058</td>\n",
       "      <td>0.500222</td>\n",
       "      <td>0.462145</td>\n",
       "      <td>0.806922</td>\n",
       "      <td>0.891912</td>\n",
       "      <td>0.276683</td>\n",
       "      <td>0.967510</td>\n",
       "      <td>0.131087</td>\n",
       "      <td>0.168836</td>\n",
       "      <td>-0.273758</td>\n",
       "      <td>1.107832</td>\n",
       "      <td>1.247795</td>\n",
       "      <td>0.482344</td>\n",
       "      <td>0.489511</td>\n",
       "      <td>-0.321138</td>\n",
       "      <td>0.573382</td>\n",
       "      <td>0.446871</td>\n",
       "      <td>-1.990581</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>188.480380</td>\n",
       "      <td>8.980801</td>\n",
       "      <td>0.760806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.548093</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.649532</td>\n",
       "      <td>0.037578</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.052079</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.496153</td>\n",
       "      <td>0.092878</td>\n",
       "      <td>0.089039</td>\n",
       "      <td>0.321334</td>\n",
       "      <td>0.549223</td>\n",
       "      <td>-0.256549</td>\n",
       "      <td>0.315373</td>\n",
       "      <td>-0.030326</td>\n",
       "      <td>-0.114335</td>\n",
       "      <td>-1.059588</td>\n",
       "      <td>-1.761360</td>\n",
       "      <td>-1.343951</td>\n",
       "      <td>-1.002550</td>\n",
       "      <td>-0.225030</td>\n",
       "      <td>-0.446653</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>61.850782</td>\n",
       "      <td>6.801283</td>\n",
       "      <td>0.322083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.612562</td>\n",
       "      <td>60.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.550496</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.350697</td>\n",
       "      <td>0.131802</td>\n",
       "      <td>0.317685</td>\n",
       "      <td>0.821415</td>\n",
       "      <td>0.581294</td>\n",
       "      <td>0.876056</td>\n",
       "      <td>0.743479</td>\n",
       "      <td>0.916367</td>\n",
       "      <td>-1.193327</td>\n",
       "      <td>-0.657307</td>\n",
       "      <td>-0.591726</td>\n",
       "      <td>-0.446856</td>\n",
       "      <td>-0.765286</td>\n",
       "      <td>-0.816544</td>\n",
       "      <td>-1.397794</td>\n",
       "      <td>-0.477130</td>\n",
       "      <td>0.810509</td>\n",
       "      <td>1.725131</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>80.946270</td>\n",
       "      <td>7.209340</td>\n",
       "      <td>0.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.073562</td>\n",
       "      <td>61.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.982447</td>\n",
       "      <td>0.487176</td>\n",
       "      <td>0.339991</td>\n",
       "      <td>0.232601</td>\n",
       "      <td>0.267207</td>\n",
       "      <td>0.800913</td>\n",
       "      <td>0.984788</td>\n",
       "      <td>0.070506</td>\n",
       "      <td>0.528945</td>\n",
       "      <td>0.005256</td>\n",
       "      <td>-1.740788</td>\n",
       "      <td>-1.778860</td>\n",
       "      <td>-0.825070</td>\n",
       "      <td>0.444932</td>\n",
       "      <td>1.173109</td>\n",
       "      <td>0.453606</td>\n",
       "      <td>-0.263440</td>\n",
       "      <td>0.246210</td>\n",
       "      <td>-0.850503</td>\n",
       "      <td>-0.412950</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>114.513000</td>\n",
       "      <td>7.829233</td>\n",
       "      <td>0.530628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.436557</td>\n",
       "      <td>62.4</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.030877</td>\n",
       "      <td>0.818540</td>\n",
       "      <td>0.955872</td>\n",
       "      <td>0.923147</td>\n",
       "      <td>0.861377</td>\n",
       "      <td>0.997349</td>\n",
       "      <td>0.091662</td>\n",
       "      <td>0.781069</td>\n",
       "      <td>0.019354</td>\n",
       "      <td>0.522191</td>\n",
       "      <td>-0.859322</td>\n",
       "      <td>1.409268</td>\n",
       "      <td>0.861992</td>\n",
       "      <td>1.109063</td>\n",
       "      <td>-1.436722</td>\n",
       "      <td>-1.461618</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.258087</td>\n",
       "      <td>0.851146</td>\n",
       "      <td>2.204813</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>137.758816</td>\n",
       "      <td>7.920083</td>\n",
       "      <td>0.604316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     outcome  depth  table        a1        a2        a3        a4        a5  \\\n",
       "0 -26.701232   62.3   56.0  0.709585  0.385796  0.267058  0.500222  0.462145   \n",
       "1   6.548093   60.5   59.0  0.649532  0.037578  0.618528  0.052079  0.008600   \n",
       "2   6.612562   60.7   58.0  0.550496  0.029469  0.350697  0.131802  0.317685   \n",
       "3  -5.073562   61.2   58.0  0.982447  0.487176  0.339991  0.232601  0.267207   \n",
       "4 -14.436557   62.4   54.0  0.030877  0.818540  0.955872  0.923147  0.861377   \n",
       "\n",
       "         b1        b2        b3        b4        b5        a6        a7  \\\n",
       "0  0.806922  0.891912  0.276683  0.967510  0.131087  0.168836 -0.273758   \n",
       "1  0.496153  0.092878  0.089039  0.321334  0.549223 -0.256549  0.315373   \n",
       "2  0.821415  0.581294  0.876056  0.743479  0.916367 -1.193327 -0.657307   \n",
       "3  0.800913  0.984788  0.070506  0.528945  0.005256 -1.740788 -1.778860   \n",
       "4  0.997349  0.091662  0.781069  0.019354  0.522191 -0.859322  1.409268   \n",
       "\n",
       "         a8        a9       a10        b6        b7        b8        b9  \\\n",
       "0  1.107832  1.247795  0.482344  0.489511 -0.321138  0.573382  0.446871   \n",
       "1 -0.030326 -0.114335 -1.059588 -1.761360 -1.343951 -1.002550 -0.225030   \n",
       "2 -0.591726 -0.446856 -0.765286 -0.816544 -1.397794 -0.477130  0.810509   \n",
       "3 -0.825070  0.444932  1.173109  0.453606 -0.263440  0.246210 -0.850503   \n",
       "4  0.861992  1.109063 -1.436722 -1.461618  0.081787  0.258087  0.851146   \n",
       "\n",
       "        b10  cut_Good  cut_Ideal  cut_Premium  cut_Very Good  color_E  \\\n",
       "0 -1.990581     False       True        False          False    False   \n",
       "1 -0.446653     False      False         True          False    False   \n",
       "2  1.725131     False      False        False           True     True   \n",
       "3 -0.412950     False      False         True          False    False   \n",
       "4  2.204813     False       True        False          False    False   \n",
       "\n",
       "   color_F  color_G  color_H  color_I  color_J  clarity_IF  clarity_SI1  \\\n",
       "0    False     True    False    False    False       False        False   \n",
       "1    False    False     True    False    False       False        False   \n",
       "2    False    False    False    False    False       False         True   \n",
       "3    False    False    False    False    False       False         True   \n",
       "4    False     True    False    False    False       False        False   \n",
       "\n",
       "   clarity_SI2  clarity_VS1  clarity_VS2  clarity_VVS1  clarity_VVS2  \\\n",
       "0        False         True        False         False         False   \n",
       "1        False        False         True         False         False   \n",
       "2        False        False        False         False         False   \n",
       "3        False        False        False         False         False   \n",
       "4         True        False        False         False         False   \n",
       "\n",
       "       volume  log_price  log_carat  \n",
       "0  188.480380   8.980801   0.760806  \n",
       "1   61.850782   6.801283   0.322083  \n",
       "2   80.946270   7.209340   0.405465  \n",
       "3  114.513000   7.829233   0.530628  \n",
       "4  137.758816   7.920083   0.604316  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded_transformed = df_encoded.copy()\n",
    "\n",
    "# Log transformation on skewed features\n",
    "df_encoded_transformed['log_price'] = np.log1p(df_encoded_transformed['price'])\n",
    "df_encoded_transformed['log_carat'] = np.log1p(df_encoded_transformed['carat'])\n",
    "\n",
    "# Drop original 'price' and 'carat' as their log-transformed versions are used\n",
    "df_encoded_transformed.drop(columns=['price', 'carat'], inplace=True)\n",
    "\n",
    "# Display transformed dataset\n",
    "df_encoded_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce5777",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a88a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_encoded_transformed = df_encoded_transformed.copy()\n",
    "\n",
    "# # Standardize numerical features\n",
    "# scaler = StandardScaler()\n",
    "# encoded_transformed_features = ['depth', 'table', 'volume', 'log_price', 'log_carat']\n",
    "# df_encoded_transformed[encoded_transformed_features] = scaler.fit_transform(df_encoded_transformed[encoded_transformed_features])\n",
    "\n",
    "# # Display encoded_transformed dataset\n",
    "# df_encoded_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df68835",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c91586e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>cut_Good</th>\n",
       "      <th>cut_Ideal</th>\n",
       "      <th>cut_Premium</th>\n",
       "      <th>cut_Very Good</th>\n",
       "      <th>color_E</th>\n",
       "      <th>color_F</th>\n",
       "      <th>color_G</th>\n",
       "      <th>color_H</th>\n",
       "      <th>color_I</th>\n",
       "      <th>color_J</th>\n",
       "      <th>clarity_IF</th>\n",
       "      <th>clarity_SI1</th>\n",
       "      <th>clarity_SI2</th>\n",
       "      <th>clarity_VS1</th>\n",
       "      <th>clarity_VS2</th>\n",
       "      <th>clarity_VVS1</th>\n",
       "      <th>clarity_VVS2</th>\n",
       "      <th>volume</th>\n",
       "      <th>log_price</th>\n",
       "      <th>log_carat</th>\n",
       "      <th>PCA_A_1</th>\n",
       "      <th>PCA_A_2</th>\n",
       "      <th>PCA_A_3</th>\n",
       "      <th>PCA_A_4</th>\n",
       "      <th>PCA_A_5</th>\n",
       "      <th>PCA_B_1</th>\n",
       "      <th>PCA_B_2</th>\n",
       "      <th>PCA_B_3</th>\n",
       "      <th>PCA_B_4</th>\n",
       "      <th>PCA_B_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-26.701232</td>\n",
       "      <td>62.3</td>\n",
       "      <td>56.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>188.480380</td>\n",
       "      <td>8.980801</td>\n",
       "      <td>0.760806</td>\n",
       "      <td>1.331813</td>\n",
       "      <td>-0.817754</td>\n",
       "      <td>0.374276</td>\n",
       "      <td>0.708130</td>\n",
       "      <td>0.344281</td>\n",
       "      <td>-0.173189</td>\n",
       "      <td>-1.073524</td>\n",
       "      <td>1.208964</td>\n",
       "      <td>-1.475998</td>\n",
       "      <td>-0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.548093</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>61.850782</td>\n",
       "      <td>6.801283</td>\n",
       "      <td>0.322083</td>\n",
       "      <td>-0.374755</td>\n",
       "      <td>0.638975</td>\n",
       "      <td>0.727730</td>\n",
       "      <td>0.142594</td>\n",
       "      <td>-0.412106</td>\n",
       "      <td>-2.046125</td>\n",
       "      <td>1.243922</td>\n",
       "      <td>0.521965</td>\n",
       "      <td>-0.034390</td>\n",
       "      <td>-0.287978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.612562</td>\n",
       "      <td>60.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>80.946270</td>\n",
       "      <td>7.209340</td>\n",
       "      <td>0.405465</td>\n",
       "      <td>-1.515717</td>\n",
       "      <td>-0.345937</td>\n",
       "      <td>0.638432</td>\n",
       "      <td>-0.040627</td>\n",
       "      <td>-0.246624</td>\n",
       "      <td>-0.154979</td>\n",
       "      <td>2.393089</td>\n",
       "      <td>-0.798952</td>\n",
       "      <td>-0.158617</td>\n",
       "      <td>0.232447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.073562</td>\n",
       "      <td>61.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>114.513000</td>\n",
       "      <td>7.829233</td>\n",
       "      <td>0.530628</td>\n",
       "      <td>-1.207823</td>\n",
       "      <td>-2.611797</td>\n",
       "      <td>-0.281070</td>\n",
       "      <td>-0.046214</td>\n",
       "      <td>0.040812</td>\n",
       "      <td>-0.386104</td>\n",
       "      <td>-0.747430</td>\n",
       "      <td>-0.029030</td>\n",
       "      <td>-0.094492</td>\n",
       "      <td>0.719731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.436557</td>\n",
       "      <td>62.4</td>\n",
       "      <td>54.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>137.758816</td>\n",
       "      <td>7.920083</td>\n",
       "      <td>0.604316</td>\n",
       "      <td>0.852174</td>\n",
       "      <td>0.485299</td>\n",
       "      <td>2.040809</td>\n",
       "      <td>0.154530</td>\n",
       "      <td>-1.287063</td>\n",
       "      <td>0.894061</td>\n",
       "      <td>2.372431</td>\n",
       "      <td>-0.146760</td>\n",
       "      <td>1.167393</td>\n",
       "      <td>-0.044095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     outcome  depth  table  cut_Good  cut_Ideal  cut_Premium  cut_Very Good  \\\n",
       "0 -26.701232   62.3   56.0     False       True        False          False   \n",
       "1   6.548093   60.5   59.0     False      False         True          False   \n",
       "2   6.612562   60.7   58.0     False      False        False           True   \n",
       "3  -5.073562   61.2   58.0     False      False         True          False   \n",
       "4 -14.436557   62.4   54.0     False       True        False          False   \n",
       "\n",
       "   color_E  color_F  color_G  color_H  color_I  color_J  clarity_IF  \\\n",
       "0    False    False     True    False    False    False       False   \n",
       "1    False    False    False     True    False    False       False   \n",
       "2     True    False    False    False    False    False       False   \n",
       "3    False    False    False    False    False    False       False   \n",
       "4    False    False     True    False    False    False       False   \n",
       "\n",
       "   clarity_SI1  clarity_SI2  clarity_VS1  clarity_VS2  clarity_VVS1  \\\n",
       "0        False        False         True        False         False   \n",
       "1        False        False        False         True         False   \n",
       "2         True        False        False        False         False   \n",
       "3         True        False        False        False         False   \n",
       "4        False         True        False        False         False   \n",
       "\n",
       "   clarity_VVS2      volume  log_price  log_carat   PCA_A_1   PCA_A_2  \\\n",
       "0         False  188.480380   8.980801   0.760806  1.331813 -0.817754   \n",
       "1         False   61.850782   6.801283   0.322083 -0.374755  0.638975   \n",
       "2         False   80.946270   7.209340   0.405465 -1.515717 -0.345937   \n",
       "3         False  114.513000   7.829233   0.530628 -1.207823 -2.611797   \n",
       "4         False  137.758816   7.920083   0.604316  0.852174  0.485299   \n",
       "\n",
       "    PCA_A_3   PCA_A_4   PCA_A_5   PCA_B_1   PCA_B_2   PCA_B_3   PCA_B_4  \\\n",
       "0  0.374276  0.708130  0.344281 -0.173189 -1.073524  1.208964 -1.475998   \n",
       "1  0.727730  0.142594 -0.412106 -2.046125  1.243922  0.521965 -0.034390   \n",
       "2  0.638432 -0.040627 -0.246624 -0.154979  2.393089 -0.798952 -0.158617   \n",
       "3 -0.281070 -0.046214  0.040812 -0.386104 -0.747430 -0.029030 -0.094492   \n",
       "4  2.040809  0.154530 -1.287063  0.894061  2.372431 -0.146760  1.167393   \n",
       "\n",
       "    PCA_B_5  \n",
       "0 -0.109100  \n",
       "1 -0.287978  \n",
       "2  0.232447  \n",
       "3  0.719731  \n",
       "4 -0.044095  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply PCA to reduce dimensions of a1-a10 and b1-b10\n",
    "pca = PCA(n_components=5)  # Keeping 5 principal components\n",
    "a_features = [f'a{i}' for i in range(1, 11)]\n",
    "b_features = [f'b{i}' for i in range(1, 11)]\n",
    "\n",
    "df_pca = df_encoded_transformed.copy()\n",
    "\n",
    "df_pca[[f'PCA_A_{i+1}' for i in range(5)]] = pca.fit_transform(df_pca[a_features])\n",
    "df_pca[[f'PCA_B_{i+1}' for i in range(5)]] = pca.fit_transform(df_pca[b_features])\n",
    "\n",
    "df_pca.drop(columns=a_features + b_features, inplace=True)\n",
    "\n",
    "# Display transformed dataset\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba560507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>a6</th>\n",
       "      <th>a7</th>\n",
       "      <th>a8</th>\n",
       "      <th>a9</th>\n",
       "      <th>a10</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>cut_Good</th>\n",
       "      <th>cut_Ideal</th>\n",
       "      <th>cut_Premium</th>\n",
       "      <th>cut_Very Good</th>\n",
       "      <th>color_E</th>\n",
       "      <th>color_F</th>\n",
       "      <th>color_G</th>\n",
       "      <th>color_H</th>\n",
       "      <th>color_I</th>\n",
       "      <th>color_J</th>\n",
       "      <th>clarity_IF</th>\n",
       "      <th>clarity_SI1</th>\n",
       "      <th>clarity_SI2</th>\n",
       "      <th>clarity_VS1</th>\n",
       "      <th>clarity_VS2</th>\n",
       "      <th>clarity_VVS1</th>\n",
       "      <th>clarity_VVS2</th>\n",
       "      <th>volume</th>\n",
       "      <th>log_price</th>\n",
       "      <th>log_carat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-26.701232</td>\n",
       "      <td>62.3</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.709585</td>\n",
       "      <td>0.385796</td>\n",
       "      <td>0.267058</td>\n",
       "      <td>0.500222</td>\n",
       "      <td>0.462145</td>\n",
       "      <td>0.806922</td>\n",
       "      <td>0.891912</td>\n",
       "      <td>0.276683</td>\n",
       "      <td>0.967510</td>\n",
       "      <td>0.131087</td>\n",
       "      <td>0.168836</td>\n",
       "      <td>-0.273758</td>\n",
       "      <td>1.107832</td>\n",
       "      <td>1.247795</td>\n",
       "      <td>0.482344</td>\n",
       "      <td>0.489511</td>\n",
       "      <td>-0.321138</td>\n",
       "      <td>0.573382</td>\n",
       "      <td>0.446871</td>\n",
       "      <td>-1.990581</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>188.480380</td>\n",
       "      <td>8.980801</td>\n",
       "      <td>0.760806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.548093</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.649532</td>\n",
       "      <td>0.037578</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.052079</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.496153</td>\n",
       "      <td>0.092878</td>\n",
       "      <td>0.089039</td>\n",
       "      <td>0.321334</td>\n",
       "      <td>0.549223</td>\n",
       "      <td>-0.256549</td>\n",
       "      <td>0.315373</td>\n",
       "      <td>-0.030326</td>\n",
       "      <td>-0.114335</td>\n",
       "      <td>-1.059588</td>\n",
       "      <td>-1.761360</td>\n",
       "      <td>-1.343951</td>\n",
       "      <td>-1.002550</td>\n",
       "      <td>-0.225030</td>\n",
       "      <td>-0.446653</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>61.850782</td>\n",
       "      <td>6.801283</td>\n",
       "      <td>0.322083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.612562</td>\n",
       "      <td>60.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.550496</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.350697</td>\n",
       "      <td>0.131802</td>\n",
       "      <td>0.317685</td>\n",
       "      <td>0.821415</td>\n",
       "      <td>0.581294</td>\n",
       "      <td>0.876056</td>\n",
       "      <td>0.743479</td>\n",
       "      <td>0.916367</td>\n",
       "      <td>-1.193327</td>\n",
       "      <td>-0.657307</td>\n",
       "      <td>-0.591726</td>\n",
       "      <td>-0.446856</td>\n",
       "      <td>-0.765286</td>\n",
       "      <td>-0.816544</td>\n",
       "      <td>-1.397794</td>\n",
       "      <td>-0.477130</td>\n",
       "      <td>0.810509</td>\n",
       "      <td>1.725131</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>80.946270</td>\n",
       "      <td>7.209340</td>\n",
       "      <td>0.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.073562</td>\n",
       "      <td>61.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.982447</td>\n",
       "      <td>0.487176</td>\n",
       "      <td>0.339991</td>\n",
       "      <td>0.232601</td>\n",
       "      <td>0.267207</td>\n",
       "      <td>0.800913</td>\n",
       "      <td>0.984788</td>\n",
       "      <td>0.070506</td>\n",
       "      <td>0.528945</td>\n",
       "      <td>0.005256</td>\n",
       "      <td>-1.740788</td>\n",
       "      <td>-1.778860</td>\n",
       "      <td>-0.825070</td>\n",
       "      <td>0.444932</td>\n",
       "      <td>1.173109</td>\n",
       "      <td>0.453606</td>\n",
       "      <td>-0.263440</td>\n",
       "      <td>0.246210</td>\n",
       "      <td>-0.850503</td>\n",
       "      <td>-0.412950</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>114.513000</td>\n",
       "      <td>7.829233</td>\n",
       "      <td>0.530628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.436557</td>\n",
       "      <td>62.4</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.030877</td>\n",
       "      <td>0.818540</td>\n",
       "      <td>0.955872</td>\n",
       "      <td>0.923147</td>\n",
       "      <td>0.861377</td>\n",
       "      <td>0.997349</td>\n",
       "      <td>0.091662</td>\n",
       "      <td>0.781069</td>\n",
       "      <td>0.019354</td>\n",
       "      <td>0.522191</td>\n",
       "      <td>-0.859322</td>\n",
       "      <td>1.409268</td>\n",
       "      <td>0.861992</td>\n",
       "      <td>1.109063</td>\n",
       "      <td>-1.436722</td>\n",
       "      <td>-1.461618</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.258087</td>\n",
       "      <td>0.851146</td>\n",
       "      <td>2.204813</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>137.758816</td>\n",
       "      <td>7.920083</td>\n",
       "      <td>0.604316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     outcome  depth  table        a1        a2        a3        a4        a5  \\\n",
       "0 -26.701232   62.3   56.0  0.709585  0.385796  0.267058  0.500222  0.462145   \n",
       "1   6.548093   60.5   59.0  0.649532  0.037578  0.618528  0.052079  0.008600   \n",
       "2   6.612562   60.7   58.0  0.550496  0.029469  0.350697  0.131802  0.317685   \n",
       "3  -5.073562   61.2   58.0  0.982447  0.487176  0.339991  0.232601  0.267207   \n",
       "4 -14.436557   62.4   54.0  0.030877  0.818540  0.955872  0.923147  0.861377   \n",
       "\n",
       "         b1        b2        b3        b4        b5        a6        a7  \\\n",
       "0  0.806922  0.891912  0.276683  0.967510  0.131087  0.168836 -0.273758   \n",
       "1  0.496153  0.092878  0.089039  0.321334  0.549223 -0.256549  0.315373   \n",
       "2  0.821415  0.581294  0.876056  0.743479  0.916367 -1.193327 -0.657307   \n",
       "3  0.800913  0.984788  0.070506  0.528945  0.005256 -1.740788 -1.778860   \n",
       "4  0.997349  0.091662  0.781069  0.019354  0.522191 -0.859322  1.409268   \n",
       "\n",
       "         a8        a9       a10        b6        b7        b8        b9  \\\n",
       "0  1.107832  1.247795  0.482344  0.489511 -0.321138  0.573382  0.446871   \n",
       "1 -0.030326 -0.114335 -1.059588 -1.761360 -1.343951 -1.002550 -0.225030   \n",
       "2 -0.591726 -0.446856 -0.765286 -0.816544 -1.397794 -0.477130  0.810509   \n",
       "3 -0.825070  0.444932  1.173109  0.453606 -0.263440  0.246210 -0.850503   \n",
       "4  0.861992  1.109063 -1.436722 -1.461618  0.081787  0.258087  0.851146   \n",
       "\n",
       "        b10  cut_Good  cut_Ideal  cut_Premium  cut_Very Good  color_E  \\\n",
       "0 -1.990581     False       True        False          False    False   \n",
       "1 -0.446653     False      False         True          False    False   \n",
       "2  1.725131     False      False        False           True     True   \n",
       "3 -0.412950     False      False         True          False    False   \n",
       "4  2.204813     False       True        False          False    False   \n",
       "\n",
       "   color_F  color_G  color_H  color_I  color_J  clarity_IF  clarity_SI1  \\\n",
       "0    False     True    False    False    False       False        False   \n",
       "1    False    False     True    False    False       False        False   \n",
       "2    False    False    False    False    False       False         True   \n",
       "3    False    False    False    False    False       False         True   \n",
       "4    False     True    False    False    False       False        False   \n",
       "\n",
       "   clarity_SI2  clarity_VS1  clarity_VS2  clarity_VVS1  clarity_VVS2  \\\n",
       "0        False         True        False         False         False   \n",
       "1        False        False         True         False         False   \n",
       "2        False        False        False         False         False   \n",
       "3        False        False        False         False         False   \n",
       "4         True        False        False         False         False   \n",
       "\n",
       "       volume  log_price  log_carat  \n",
       "0  188.480380   8.980801   0.760806  \n",
       "1   61.850782   6.801283   0.322083  \n",
       "2   80.946270   7.209340   0.405465  \n",
       "3  114.513000   7.829233   0.530628  \n",
       "4  137.758816   7.920083   0.604316  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8efee269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_reduced = df_pca.drop(columns=['outcome'])\n",
    "\n",
    "# # Perform cross-validation on reduced features\n",
    "# for name, model in models.items():\n",
    "#     score = cross_val_score(model, X_reduced, y, cv=5, scoring='r2').mean()\n",
    "#     print(f\"{name} R Score (Reduced Features with PCA): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c76c6d",
   "metadata": {},
   "source": [
    "## Check for Low-Variance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10c63ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Low-Variance Features: set()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Apply variance threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_var_filtered = selector.fit_transform(df_encoded_transformed.drop(columns=['outcome']))\n",
    "\n",
    "# Get remaining feature names\n",
    "selected_features = df_encoded_transformed.drop(columns=['outcome']).columns[selector.get_support()]\n",
    "\n",
    "# Display removed features\n",
    "removed_features = set(df_encoded_transformed.drop(columns=['outcome']).columns) - set(selected_features)\n",
    "print(\"Removed Low-Variance Features:\", removed_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336edbe",
   "metadata": {},
   "source": [
    "## Mutual information (Information Gain) for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec0a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Features: ['cut_Ideal', 'color_F', 'cut_Very Good', 'clarity_IF', 'cut_Premium', 'clarity_SI2', 'clarity_VS2', 'clarity_VVS1']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>a6</th>\n",
       "      <th>a7</th>\n",
       "      <th>a8</th>\n",
       "      <th>a9</th>\n",
       "      <th>a10</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>cut_Good</th>\n",
       "      <th>color_E</th>\n",
       "      <th>color_G</th>\n",
       "      <th>color_H</th>\n",
       "      <th>color_I</th>\n",
       "      <th>color_J</th>\n",
       "      <th>clarity_SI1</th>\n",
       "      <th>clarity_VS1</th>\n",
       "      <th>clarity_VVS2</th>\n",
       "      <th>volume</th>\n",
       "      <th>log_price</th>\n",
       "      <th>log_carat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-26.701232</td>\n",
       "      <td>62.3</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.709585</td>\n",
       "      <td>0.385796</td>\n",
       "      <td>0.267058</td>\n",
       "      <td>0.500222</td>\n",
       "      <td>0.462145</td>\n",
       "      <td>0.806922</td>\n",
       "      <td>0.891912</td>\n",
       "      <td>0.276683</td>\n",
       "      <td>0.967510</td>\n",
       "      <td>0.131087</td>\n",
       "      <td>0.168836</td>\n",
       "      <td>-0.273758</td>\n",
       "      <td>1.107832</td>\n",
       "      <td>1.247795</td>\n",
       "      <td>0.482344</td>\n",
       "      <td>0.489511</td>\n",
       "      <td>-0.321138</td>\n",
       "      <td>0.573382</td>\n",
       "      <td>0.446871</td>\n",
       "      <td>-1.990581</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>188.480380</td>\n",
       "      <td>8.980801</td>\n",
       "      <td>0.760806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.548093</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.649532</td>\n",
       "      <td>0.037578</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.052079</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.496153</td>\n",
       "      <td>0.092878</td>\n",
       "      <td>0.089039</td>\n",
       "      <td>0.321334</td>\n",
       "      <td>0.549223</td>\n",
       "      <td>-0.256549</td>\n",
       "      <td>0.315373</td>\n",
       "      <td>-0.030326</td>\n",
       "      <td>-0.114335</td>\n",
       "      <td>-1.059588</td>\n",
       "      <td>-1.761360</td>\n",
       "      <td>-1.343951</td>\n",
       "      <td>-1.002550</td>\n",
       "      <td>-0.225030</td>\n",
       "      <td>-0.446653</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>61.850782</td>\n",
       "      <td>6.801283</td>\n",
       "      <td>0.322083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.612562</td>\n",
       "      <td>60.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.550496</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.350697</td>\n",
       "      <td>0.131802</td>\n",
       "      <td>0.317685</td>\n",
       "      <td>0.821415</td>\n",
       "      <td>0.581294</td>\n",
       "      <td>0.876056</td>\n",
       "      <td>0.743479</td>\n",
       "      <td>0.916367</td>\n",
       "      <td>-1.193327</td>\n",
       "      <td>-0.657307</td>\n",
       "      <td>-0.591726</td>\n",
       "      <td>-0.446856</td>\n",
       "      <td>-0.765286</td>\n",
       "      <td>-0.816544</td>\n",
       "      <td>-1.397794</td>\n",
       "      <td>-0.477130</td>\n",
       "      <td>0.810509</td>\n",
       "      <td>1.725131</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>80.946270</td>\n",
       "      <td>7.209340</td>\n",
       "      <td>0.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.073562</td>\n",
       "      <td>61.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.982447</td>\n",
       "      <td>0.487176</td>\n",
       "      <td>0.339991</td>\n",
       "      <td>0.232601</td>\n",
       "      <td>0.267207</td>\n",
       "      <td>0.800913</td>\n",
       "      <td>0.984788</td>\n",
       "      <td>0.070506</td>\n",
       "      <td>0.528945</td>\n",
       "      <td>0.005256</td>\n",
       "      <td>-1.740788</td>\n",
       "      <td>-1.778860</td>\n",
       "      <td>-0.825070</td>\n",
       "      <td>0.444932</td>\n",
       "      <td>1.173109</td>\n",
       "      <td>0.453606</td>\n",
       "      <td>-0.263440</td>\n",
       "      <td>0.246210</td>\n",
       "      <td>-0.850503</td>\n",
       "      <td>-0.412950</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>114.513000</td>\n",
       "      <td>7.829233</td>\n",
       "      <td>0.530628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.436557</td>\n",
       "      <td>62.4</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.030877</td>\n",
       "      <td>0.818540</td>\n",
       "      <td>0.955872</td>\n",
       "      <td>0.923147</td>\n",
       "      <td>0.861377</td>\n",
       "      <td>0.997349</td>\n",
       "      <td>0.091662</td>\n",
       "      <td>0.781069</td>\n",
       "      <td>0.019354</td>\n",
       "      <td>0.522191</td>\n",
       "      <td>-0.859322</td>\n",
       "      <td>1.409268</td>\n",
       "      <td>0.861992</td>\n",
       "      <td>1.109063</td>\n",
       "      <td>-1.436722</td>\n",
       "      <td>-1.461618</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.258087</td>\n",
       "      <td>0.851146</td>\n",
       "      <td>2.204813</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>137.758816</td>\n",
       "      <td>7.920083</td>\n",
       "      <td>0.604316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     outcome  depth  table        a1        a2        a3        a4        a5  \\\n",
       "0 -26.701232   62.3   56.0  0.709585  0.385796  0.267058  0.500222  0.462145   \n",
       "1   6.548093   60.5   59.0  0.649532  0.037578  0.618528  0.052079  0.008600   \n",
       "2   6.612562   60.7   58.0  0.550496  0.029469  0.350697  0.131802  0.317685   \n",
       "3  -5.073562   61.2   58.0  0.982447  0.487176  0.339991  0.232601  0.267207   \n",
       "4 -14.436557   62.4   54.0  0.030877  0.818540  0.955872  0.923147  0.861377   \n",
       "\n",
       "         b1        b2        b3        b4        b5        a6        a7  \\\n",
       "0  0.806922  0.891912  0.276683  0.967510  0.131087  0.168836 -0.273758   \n",
       "1  0.496153  0.092878  0.089039  0.321334  0.549223 -0.256549  0.315373   \n",
       "2  0.821415  0.581294  0.876056  0.743479  0.916367 -1.193327 -0.657307   \n",
       "3  0.800913  0.984788  0.070506  0.528945  0.005256 -1.740788 -1.778860   \n",
       "4  0.997349  0.091662  0.781069  0.019354  0.522191 -0.859322  1.409268   \n",
       "\n",
       "         a8        a9       a10        b6        b7        b8        b9  \\\n",
       "0  1.107832  1.247795  0.482344  0.489511 -0.321138  0.573382  0.446871   \n",
       "1 -0.030326 -0.114335 -1.059588 -1.761360 -1.343951 -1.002550 -0.225030   \n",
       "2 -0.591726 -0.446856 -0.765286 -0.816544 -1.397794 -0.477130  0.810509   \n",
       "3 -0.825070  0.444932  1.173109  0.453606 -0.263440  0.246210 -0.850503   \n",
       "4  0.861992  1.109063 -1.436722 -1.461618  0.081787  0.258087  0.851146   \n",
       "\n",
       "        b10  cut_Good  color_E  color_G  color_H  color_I  color_J  \\\n",
       "0 -1.990581     False    False     True    False    False    False   \n",
       "1 -0.446653     False    False    False     True    False    False   \n",
       "2  1.725131     False     True    False    False    False    False   \n",
       "3 -0.412950     False    False    False    False    False    False   \n",
       "4  2.204813     False    False     True    False    False    False   \n",
       "\n",
       "   clarity_SI1  clarity_VS1  clarity_VVS2      volume  log_price  log_carat  \n",
       "0        False         True         False  188.480380   8.980801   0.760806  \n",
       "1        False        False         False   61.850782   6.801283   0.322083  \n",
       "2         True        False         False   80.946270   7.209340   0.405465  \n",
       "3         True        False         False  114.513000   7.829233   0.530628  \n",
       "4        False        False         False  137.758816   7.920083   0.604316  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select categorical features\n",
    "categorical_features = [col for col in df_encoded_transformed.columns if 'cut' in col or 'color' in col or 'clarity' in col]\n",
    "\n",
    "# Apply MinMax Scaling (required for mutual information)\n",
    "X_encoded_transformed = MinMaxScaler().fit_transform(df_encoded_transformed[categorical_features])\n",
    "\n",
    "# Compute Mutual Information (Information Gain)\n",
    "info_gain = mutual_info_regression(X_encoded_transformed, df_encoded_transformed['outcome'])\n",
    "\n",
    "# Create DataFrame of results\n",
    "info_gain_df = pd.DataFrame({'Feature': categorical_features, 'Information Gain': info_gain})\n",
    "info_gain_df = info_gain_df.sort_values(by='Information Gain', ascending=False)\n",
    "\n",
    "# Set threshold for removal (default: IG < 0.001)\n",
    "threshold = 0.001\n",
    "low_ig_features = info_gain_df[info_gain_df['Information Gain'] < threshold]['Feature'].tolist()\n",
    "\n",
    "df_reduced_IG = df_encoded_transformed.copy()\n",
    "\n",
    "# Drop low-information features\n",
    "df_reduced_IG.drop(columns=low_ig_features, inplace=True)\n",
    "\n",
    "# Display remaining features\n",
    "print(f\"Removed Features: {low_ig_features}\")\n",
    "df_reduced_IG.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cf1bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_reduced = df_reduced_IG.drop(columns=['outcome'])\n",
    "\n",
    "# # Perform cross-validation on reduced features\n",
    "# for name, model in models.items():\n",
    "#     score = cross_val_score(model, X_reduced, y, cv=5, scoring='r2').mean()\n",
    "#     print(f\"{name} R Score (Reduced Features with Mutual info regression): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c2c466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed dataset\n",
    "df_encoded_transformed.to_csv(\"CW1_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d52ceed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dataset saved as 'CW1_Reduced.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Reduced as these have no linear or no non linear features with outcome\n",
    "df_transformed = pd.read_csv('CW1_transformed.csv')\n",
    "\n",
    "df_Reduced = df.drop(columns=['a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4', 'b5'])\n",
    "\n",
    "df_Reduced.to_csv('CW1_Reduced.csv', index=False)\n",
    "print(\"Reduced dataset saved as 'CW1_Reduced.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52b8ee",
   "metadata": {},
   "source": [
    "## Feature Reduction with Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06621aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with outcome:\n",
      "outcome      1.000000\n",
      "log_price    0.005783\n",
      "log_carat   -0.000585\n",
      "Name: outcome, dtype: float64\n",
      "\n",
      "Correlation among correlated features:\n",
      "           log_price  log_carat\n",
      "log_price   1.000000   0.952355\n",
      "log_carat   0.952355   1.000000\n"
     ]
    }
   ],
   "source": [
    "df_reduced_Correlation = df_reduced_IG.copy()\n",
    "\n",
    "def correlation (dataset, threshold):\n",
    "    col_corr = set() # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value.\n",
    "                colname = corr_matrix.columns[i] # getting the name of column\n",
    "                col_corr.add(colname )\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(df_reduced_Correlation, 0.7)\n",
    "# Convert set to list before concatenating\n",
    "corr_matrix = df_reduced_Correlation[['outcome'] + list(corr_features)].corr()\n",
    "\n",
    "# Display correlation of each feature with outcome\n",
    "print(\"Correlation with outcome:\")\n",
    "print(corr_matrix['outcome'].sort_values(ascending=False))\n",
    "\n",
    "# Display correlation among correlated features\n",
    "print(\"\\nCorrelation among correlated features:\")\n",
    "print(corr_matrix.loc[list(corr_features), list(corr_features)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49b618",
   "metadata": {},
   "source": [
    "### Feature Selection with RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35fa3651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: Index(['depth', 'table', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9',\n",
      "       'b10', 'cut_Very Good', 'color_F', 'color_G', 'color_H', 'clarity_SI1',\n",
      "       'clarity_SI2', 'clarity_VS1', 'clarity_VS2', 'volume', 'log_price',\n",
      "       'log_carat'],\n",
      "      dtype='object')\n",
      "R Score (RFECV): 0.3057\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Selected Features: 1 -> Features: ['depth']\n",
      "Number of Selected Features: 2 -> Features: ['depth', 'volume']\n",
      "Number of Selected Features: 3 -> Features: ['depth', 'volume', 'clarity_VS2']\n",
      "Number of Selected Features: 4 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1']\n",
      "Number of Selected Features: 5 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2']\n",
      "Number of Selected Features: 6 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1']\n",
      "Number of Selected Features: 7 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H']\n",
      "Number of Selected Features: 8 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G']\n",
      "Number of Selected Features: 9 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F']\n",
      "Number of Selected Features: 10 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price']\n",
      "Number of Selected Features: 11 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10']\n",
      "Number of Selected Features: 12 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good']\n",
      "Number of Selected Features: 13 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat']\n",
      "Number of Selected Features: 14 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8']\n",
      "Number of Selected Features: 15 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7']\n",
      "Number of Selected Features: 16 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6']\n",
      "Number of Selected Features: 17 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10']\n",
      "Number of Selected Features: 18 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6']\n",
      "Number of Selected Features: 19 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9']\n",
      "Number of Selected Features: 20 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table']\n",
      "Number of Selected Features: 21 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9']\n",
      "Number of Selected Features: 22 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8']\n",
      "Number of Selected Features: 23 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8', 'a7']\n",
      "Number of Selected Features: 24 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8', 'a7', 'color_E']\n",
      "Number of Selected Features: 25 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8', 'a7', 'color_E', 'cut_Premium']\n",
      "Number of Selected Features: 26 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8', 'a7', 'color_E', 'cut_Premium', 'cut_Ideal']\n",
      "Number of Selected Features: 27 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8', 'a7', 'color_E', 'cut_Premium', 'cut_Ideal', 'clarity_VVS2']\n",
      "Number of Selected Features: 28 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8', 'a7', 'color_E', 'cut_Premium', 'cut_Ideal', 'clarity_VVS2', 'clarity_VVS1']\n",
      "Number of Selected Features: 29 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8', 'a7', 'color_E', 'cut_Premium', 'cut_Ideal', 'clarity_VVS2', 'clarity_VVS1', 'color_I']\n",
      "Number of Selected Features: 30 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8', 'a7', 'color_E', 'cut_Premium', 'cut_Ideal', 'clarity_VVS2', 'clarity_VVS1', 'color_I', 'color_J']\n",
      "Number of Selected Features: 31 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8', 'a7', 'color_E', 'cut_Premium', 'cut_Ideal', 'clarity_VVS2', 'clarity_VVS1', 'color_I', 'color_J', 'cut_Good']\n",
      "Number of Selected Features: 32 -> Features: ['depth', 'volume', 'clarity_VS2', 'clarity_VS1', 'clarity_SI2', 'clarity_SI1', 'color_H', 'color_G', 'color_F', 'log_price', 'b10', 'cut_Very Good', 'log_carat', 'b8', 'b7', 'b6', 'a10', 'a6', 'b9', 'table', 'a9', 'a8', 'a7', 'color_E', 'cut_Premium', 'cut_Ideal', 'clarity_VVS2', 'clarity_VVS1', 'color_I', 'color_J', 'cut_Good', 'clarity_IF']\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary libraries since execution state was reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# import ace_tools as tools\n",
    "\n",
    "# Reload dataset\n",
    "df = pd.read_csv(\"CW1_Reduced.csv\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_encoded = pd.DataFrame(encoder.fit_transform(df[categorical_cols]))\n",
    "\n",
    "# Assign proper column names\n",
    "X_encoded.columns = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Drop original categorical columns and concatenate encoded data\n",
    "X = df.drop(columns=categorical_cols).drop(columns=['outcome']).reset_index(drop=True)\n",
    "X_encoded = X_encoded.reset_index(drop=True)\n",
    "X_final = pd.concat([X, X_encoded], axis=1)\n",
    "y = df['outcome']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Recursive Feature Elimination with Cross-Validation\n",
    "rfecv = RFECV(estimator=model, step=1, cv=5, scoring='r2')\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X_final.columns[rfecv.support_]\n",
    "\n",
    "# Train model on selected features\n",
    "model.fit(X_train[selected_features], y_train)\n",
    "y_pred = model.predict(X_test[selected_features])\n",
    "\n",
    "# Evaluate performance\n",
    "best_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Selected Features: {selected_features}\")\n",
    "print(f\"R Score (RFECV): {best_r2:.4f}\")\n",
    "\n",
    "# Fix: Use cv_results_ instead of grid_scores_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'], marker='o')\n",
    "plt.xlabel('Number of Selected Features')\n",
    "plt.ylabel('Cross-Validated R2 Score')\n",
    "plt.title('Feature Selection with RFECV')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a dictionary to store the number of selected features at each step\n",
    "feature_selection_dict = {}\n",
    "\n",
    "# Iterate through the number of selected features at each step\n",
    "for n_features in range(1, len(X_train.columns) + 1):\n",
    "    # Get the top 'n_features' based on the ranking\n",
    "    selected_features_at_step = X_train.columns[np.argsort(rfecv.ranking_)[:n_features]]\n",
    "    feature_selection_dict[n_features] = list(selected_features_at_step)\n",
    "\n",
    "# Display feature selection results correctly\n",
    "for k, v in feature_selection_dict.items():\n",
    "    print(f\"Number of Selected Features: {k} -> Features: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d9a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['depth', 'b3', 'b1', 'a1', 'a4', 'a3', 'a2', 'b5', 'a6', 'b10', 'b2', 'b9', 'a7', 'b6', 'a10', 'log_price']\n",
      "R Score (RFECV): 0.4266\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"CW1_transformed.csv\")  # Change to your actual file path\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_encoded = pd.DataFrame(encoder.fit_transform(df[categorical_cols]))\n",
    "\n",
    "# Assign proper column names\n",
    "X_encoded.columns = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Drop original categorical columns and concatenate encoded data\n",
    "X = df.drop(columns=categorical_cols).drop(columns=['outcome']).reset_index(drop=True)\n",
    "X_encoded = X_encoded.reset_index(drop=True)\n",
    "X_final = pd.concat([X, X_encoded], axis=1)\n",
    "y = df['outcome']\n",
    "\n",
    "# Ensure all values are numeric\n",
    "X_final = X_final.apply(pd.to_numeric)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Step 1: Feature Importance with Random Forest\n",
    "model.fit(X_train, y_train)\n",
    "feature_importances = pd.DataFrame({'Feature': X_final.columns, 'Importance': model.feature_importances_})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Filter only important features (set threshold)\n",
    "important_features = feature_importances[feature_importances['Importance'] > 0.01]['Feature'].tolist()\n",
    "X_train_filtered = X_train[important_features]\n",
    "X_test_filtered = X_test[important_features]\n",
    "\n",
    "# Step 2: Recursive Feature Elimination with Cross-Validation (RFECV)\n",
    "rfecv = RFECV(estimator=model, step=2, min_features_to_select=10, cv=5, scoring='r2')\n",
    "rfecv.fit(X_train_filtered, y_train)\n",
    "\n",
    "# Select final features\n",
    "selected_features = X_train_filtered.columns[rfecv.support_]\n",
    "X_train_selected = X_train_filtered[selected_features]\n",
    "X_test_selected = X_test_filtered[selected_features]\n",
    "\n",
    "# Train model on selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate performance\n",
    "best_r2_RFECV = r2_score(y_test, y_pred)\n",
    "print(f\"Selected Features: {list(selected_features)}\")\n",
    "print(f\"R Score (RFECV): {best_r2_RFECV:.4f}\")\n",
    "\n",
    "# Plot RFECV results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'], marker='o')\n",
    "plt.xlabel('Number of Selected Features')\n",
    "plt.ylabel('Cross-Validated R2 Score')\n",
    "plt.title('Feature Selection with RFECV')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ee9a7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['depth', 'b3', 'b1', 'a1', 'a4', 'a3', 'a2', 'b5', 'a6', 'b10', 'b2',\n",
       "       'b9', 'a7', 'b6', 'a10', 'log_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_selected.columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply PCA for Dimensionality Reduction (if needed)\n",
    "pca = PCA(n_components=10)  # Reduce to 10 principal components\n",
    "X_train_pca = pca.fit_transform(X_train_selected)\n",
    "X_test_pca = pca.transform(X_test_selected)\n",
    "\n",
    "# Train and evaluate model using PCA-reduced features\n",
    "model.fit(X_train_pca, y_train)\n",
    "y_pred_pca = model.predict(X_test_pca)\n",
    "pca_r2 = r2_score(y_test, y_pred_pca)\n",
    "\n",
    "print(f\"PCA-Reduced R Score: {pca_r2:.4f}\")\n",
    "\n",
    "# Display top feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances['Feature'][:10], feature_importances['Importance'][:10])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 10 Feature Importances from Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69105e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R Score: 0.3057\n",
      "Selected Features:\n",
      "['depth', 'table', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10', 'cut_Very Good', 'color_F', 'color_G', 'color_H', 'clarity_SI1', 'clarity_SI2', 'clarity_VS1', 'clarity_VS2', 'volume', 'log_price', 'log_carat']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best R Score: {best_r2:.4f}\")\n",
    "print(\"Selected Features:\")\n",
    "print(list(selected_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30515fa8",
   "metadata": {},
   "source": [
    "### Feature Selection with SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89867123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Selected Features: ['depth', 'b3', 'b1', 'a1', 'a4', 'a3', 'a2', 'b5', 'a6', 'b10', 'b2', 'b9', 'a5', 'a7', 'b6']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'k_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop Selected Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m, top_features)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Apply Sequential Feature Selection (SFS) on reduced set\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m sfs \u001b[38;5;241m=\u001b[39m \u001b[43mSequentialFeatureSelector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloating\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m sfs\u001b[38;5;241m.\u001b[39mfit(X_train[top_features], y_train)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Print the best selected features\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'k_features'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"CW1_transformed.csv\")  \n",
    "\n",
    "# Encoding categorical features\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Define features and target variable\n",
    "X = df_encoded.drop(columns=['outcome'])\n",
    "y = df_encoded['outcome']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RandomForest to get feature importance\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Select top 15 features\n",
    "top_features = feature_importance_df['Feature'].iloc[:15].tolist()\n",
    "print(\"Top Selected Features:\", top_features)\n",
    "\n",
    "# Apply Sequential Feature Selection (SFS) on reduced set\n",
    "sfs = SequentialFeatureSelector(model, k_features=5, forward=True, floating=False, scoring='r2', cv=3, n_jobs=-1)\n",
    "sfs.fit(X_train[top_features], y_train)\n",
    "\n",
    "# Print the best selected features\n",
    "print(\"Best Selected Features:\", sfs.k_feature_names_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ce7f8",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358e322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R Score (Reduced Features by SFS): 0.2851\n",
      "Random Forest R Score (Reduced Features by SFS): 0.4406\n",
      "XGBoost R Score (Reduced Features by SFS): 0.3988\n",
      "Linear Regression R Score (Reduced Features by SFS): 0.2851\n"
     ]
    }
   ],
   "source": [
    "selected_features_with_SFS = ['depth', 'b3', 'b1', 'a1', 'a4', 'a3', 'b6']\n",
    "\n",
    "df_selected = df[selected_features_with_SFS]\n",
    "\n",
    "# Perform cross-validation on reduced features\n",
    "for name, model in models.items():\n",
    "    score = cross_val_score(model, df_selected, y, cv=5, scoring='r2').mean()\n",
    "    print(f\"{name} R Score (Reduced Features by SFS): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19600759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R Score (Reduced Features by rfecv): 0.2841\n",
      "Random Forest R Score (Reduced Features by rfecv): 0.4427\n",
      "XGBoost R Score (Reduced Features by rfecv): 0.3937\n",
      "Linear Regression R Score (Reduced Features by rfecv): 0.2841\n"
     ]
    }
   ],
   "source": [
    "selected_features_with_RFECV = ['depth', 'b3', 'b1', 'a1', 'a4', 'a3', 'a2', 'b5', 'b10', 'b9', 'a7', 'a10', 'price']\n",
    "\n",
    "df_selected = df[selected_features_with_RFECV]\n",
    "\n",
    "# Perform cross-validation on reduced features\n",
    "for name, model in models.items():\n",
    "    score = cross_val_score(model, df_selected, y, cv=5, scoring='r2').mean()\n",
    "    print(f\"{name} R Score (Reduced Features by rfecv): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a248eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R Score (Reduced Features by rfecv): 0.2860\n",
      "Random Forest R Score (Reduced Features by rfecv): 0.4418\n",
      "XGBoost R Score (Reduced Features by rfecv): 0.3980\n",
      "Linear Regression R Score (Reduced Features by rfecv): 0.2860\n"
     ]
    }
   ],
   "source": [
    "selected_features_with_RFECV_more_Features = ['depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4',\n",
    "       'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10',\n",
    "       'cut_Good', 'cut_Ideal', 'cut_Premium', 'cut_Very Good', 'color_E',\n",
    "       'color_F', 'color_G', 'color_H', 'color_I', 'color_J', 'clarity_SI1',\n",
    "       'clarity_SI2', 'clarity_VS1', 'clarity_VS2', 'clarity_VVS1',\n",
    "       'clarity_VVS2', 'volume', 'log_price', 'log_carat']\n",
    "\n",
    "\n",
    "df_selected = df_encoded_transformed[selected_features_with_RFECV_more_Features].copy()\n",
    "\n",
    "# Perform cross-validation on reduced features\n",
    "for name, model in models.items():\n",
    "    score = cross_val_score(model, df_selected, y, cv=5, scoring='r2').mean()\n",
    "    print(f\"{name} R Score (All Features by rfecv): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebb334",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76cf7366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=250; total time=   3.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=250; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=250; total time=   4.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=400; total time=   5.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=400; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   7.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=300; total time=   4.4s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=300; total time=   4.5s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=400; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=300; total time=   4.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=350; total time=   4.6s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=350; total time=   4.2s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=300; total time=   4.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=350; total time=   4.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=300; total time=   4.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=300; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=350; total time=   6.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=350; total time=   6.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=350; total time=   6.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=250; total time=   3.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=250; total time=   3.8s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=350; total time=   5.9s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=350; total time=   6.1s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=350; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=250; total time=   3.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=20, n_estimators=250; total time=   3.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=20, n_estimators=250; total time=   3.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=20, n_estimators=250; total time=   3.6s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=300; total time=   3.7s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=300; total time=   3.6s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=300; total time=   3.6s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   5.5s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=400; total time=   5.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=400; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=400; total time=   5.5s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=400; total time=   5.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=400; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=400; total time=   5.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   4.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=400; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=400; total time=   5.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=400; total time=   5.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time=   7.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=300; total time=   4.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=300; total time=   4.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=300; total time=   4.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=350; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=350; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=350; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   4.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   4.3s\n",
      "\n",
      "Best Hyperparameters for Random Forest:\n",
      " {'n_estimators': 350, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2', 'max_depth': 15, 'bootstrap': True}\n",
      "Final R Score: 0.4437\n",
      "Final MSE: 90.6775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Select features and target\n",
    "df_selected_features = df_transformed[['outcome','depth', 'b3', 'b1', 'a1', 'a4', 'a3', 'a2', 'b5', 'a6', 'b10', 'b2',\n",
    "       'b9', 'a7', 'b6', 'a10', 'log_price']]\n",
    "\n",
    "target_col = \"outcome\"  \n",
    "X = df_selected_features.drop(columns=[target_col])  \n",
    "y = df_selected_features[target_col]  \n",
    "\n",
    "# Train-Test Split (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a wider hyperparameter search space for better performance\n",
    "param_dist = {\n",
    "   \"n_estimators\":[250, 300, 350, 400],  # More trees for better performance\n",
    "    \"max_depth\": [10, 15, 20, 25, None],  # Allow deeper trees if beneficial\n",
    "    \"min_samples_split\": [2, 5, 10, 15, 20],  # More granularity\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10],  # Helps regularization\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],  # Feature selection strategy\n",
    "    \"bootstrap\": [True]  # Keep bootstrap enabled\n",
    "}\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV (Increased n_iter and CV for better performance)\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=20,  # Increased from 10 to 20 for better tuning\n",
    "    cv=3,  # Increased from 2 to 3 for better generalization\n",
    "    scoring='r2', \n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42,\n",
    "    verbose=2  # More detailed progress output\n",
    ")\n",
    "\n",
    "# Fit hyperparameter search on training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the search\n",
    "best_params = random_search.best_params_\n",
    "final_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate model on validation set\n",
    "y_pred_final = final_model.predict(X_val)\n",
    "final_r2 = r2_score(y_val, y_pred_final)\n",
    "final_mse = mean_squared_error(y_val, y_pred_final)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest Hyperparameters for Random Forest:\\n\", best_params)\n",
    "print(f\"Final R Score: {final_r2:.4f}\")\n",
    "print(f\"Final MSE: {final_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fa93467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=250; total time=   5.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=250; total time=   5.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=250; total time=   5.4s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=250; total time=   5.5s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=250; total time=   5.5s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=350; total time=   8.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=350; total time=   8.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=350; total time=   8.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=250; total time=   4.7s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=350; total time=   8.1s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=350; total time=   8.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   8.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   8.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   9.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   8.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=400; total time=   8.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=250; total time=   4.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=250; total time=   4.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=250; total time=   4.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=250; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   5.3s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=250; total time=   4.6s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   5.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=250; total time=   4.6s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   5.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=250; total time=   4.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=250; total time=   4.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=250; total time=   4.6s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300; total time=   5.8s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300; total time=   5.8s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300; total time=   6.0s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300; total time=   5.9s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300; total time=   5.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350; total time=   7.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350; total time=   7.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350; total time=   7.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=400; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=400; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=400; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   6.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=400; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=400; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   6.4s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=350; total time=   6.8s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300; total time=   7.1s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300; total time=   7.1s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300; total time=   6.7s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=350; total time=   7.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=350; total time=   7.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=350; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=350; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=350; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time=   9.4s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time=   9.7s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   6.4s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   6.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time=   9.5s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time=   9.6s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   6.2s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time=   9.6s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=350; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=350; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=350; total time=   7.7s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=350; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=350; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=400; total time=   7.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=400; total time=   7.7s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=400; total time=   8.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=400; total time=   7.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   5.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   5.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=400; total time=   8.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   5.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=250; total time=   5.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=250; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=250; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=250; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=250; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=250; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=400; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=400; total time=   7.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=400; total time=   7.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=400; total time=   8.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=400; total time=   7.8s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=400; total time=   9.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=400; total time=   9.2s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=400; total time=   9.6s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=400; total time=   9.3s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=5, min_samples_split=3, n_estimators=350; total time=   6.4s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=400; total time=   9.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=5, min_samples_split=3, n_estimators=350; total time=   6.5s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=5, min_samples_split=3, n_estimators=350; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=5, min_samples_split=3, n_estimators=350; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=5, min_samples_split=3, n_estimators=350; total time=   6.4s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=400; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=400; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   6.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=400; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=400; total time=   7.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=400; total time=   7.2s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   6.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   6.5s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=300; total time=   5.9s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=300; total time=   6.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=300; total time=   6.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   6.9s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=300; total time=   6.6s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=300; total time=   6.3s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   9.6s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   9.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   8.2s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   8.7s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   8.5s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=400; total time=   8.8s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=400; total time=   9.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=400; total time=   8.9s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=400; total time=   8.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   7.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=400; total time=   8.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=250; total time=   5.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=250; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=250; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=250; total time=   5.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=250; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=250; total time=   5.1s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=250; total time=   5.5s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=250; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=250; total time=   5.2s\n",
      "[CV] END bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=250; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=250; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=250; total time=   5.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=250; total time=   4.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=250; total time=   4.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=250; total time=   4.6s\n",
      "\n",
      "Best Hyperparameters for Random Forest:\n",
      " {'n_estimators': 350, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': True}\n",
      "Final R Score: 0.4419\n",
      "Final MSE: 90.9684\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Select features and target\n",
    "df_selected_features = df_transformed[['outcome','depth', 'b3', 'b1', 'a1', 'a4', 'a3', 'a2', 'b5', 'a6', 'b10', 'b2',\n",
    "       'b9', 'a7', 'b6', 'a10', 'log_price']]\n",
    "\n",
    "target_col = \"outcome\"  \n",
    "X = df_selected_features.drop(columns=[target_col])  \n",
    "y = df_selected_features[target_col]  \n",
    "\n",
    "# Train-Test Split (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Expanded hyperparameter search space\n",
    "param_dist = {\n",
    "    \"n_estimators\": [250, 300, 350, 400],  # More trees for better accuracy\n",
    "    \"max_depth\": [15, 20, 25, 30],  # Allow deeper trees\n",
    "    \"min_samples_split\": [2, 3, 5],  # Allow more splits\n",
    "    \"min_samples_leaf\": [1, 2, 3, 5],  # Allow smaller leaf sizes\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],  # Feature selection strategy\n",
    "    \"bootstrap\": [True]  # Keep bootstrap enabled\n",
    "}\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV with more iterations and better validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=30,  # Increased from 20 to 30\n",
    "    cv=5,  # More robust cross-validation\n",
    "    scoring='r2', \n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42,\n",
    "    verbose=2  # Show detailed progress\n",
    ")\n",
    "\n",
    "# Fit hyperparameter search on training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the search\n",
    "best_params = random_search.best_params_\n",
    "final_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate model on validation set\n",
    "y_pred_final = final_model.predict(X_val)\n",
    "final_r2 = r2_score(y_val, y_pred_final)\n",
    "final_mse = mean_squared_error(y_val, y_pred_final)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest Hyperparameters for Random Forest:\\n\", best_params)\n",
    "print(f\"Final R Score: {final_r2:.4f}\")\n",
    "print(f\"Final MSE: {final_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "322affbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 30 candidates, totalling 210 fits\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   8.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   9.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   9.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   9.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   9.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   9.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   9.7s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   9.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   9.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   9.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   9.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   9.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=  10.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=  10.1s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=350, warm_start=True; total time=  10.9s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=350, warm_start=True; total time=  11.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=350, warm_start=True; total time=  10.3s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=350, warm_start=True; total time=  10.7s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=350, warm_start=True; total time=  10.4s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=350, warm_start=True; total time=  10.7s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=350, warm_start=True; total time=  10.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  13.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  13.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  14.5s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=  11.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  14.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  13.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  13.7s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  13.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=  11.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=  11.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=  10.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=  10.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   9.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=  10.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  11.8s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  12.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  12.1s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  12.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  12.6s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  12.6s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  13.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  12.8s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  12.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  13.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  12.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500, warm_start=True; total time=  15.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500, warm_start=True; total time=  16.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500, warm_start=True; total time=  17.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  14.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=500, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500, warm_start=True; total time=  17.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  15.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500, warm_start=True; total time=  18.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=450, warm_start=True; total time=  15.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500, warm_start=True; total time=  17.5s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500, warm_start=True; total time=  17.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=450, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=25, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=350, warm_start=True; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=350, warm_start=True; total time=  10.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=350, warm_start=True; total time=  11.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=350, warm_start=True; total time=  10.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=350, warm_start=True; total time=  10.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=350, warm_start=True; total time=  10.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=350, warm_start=True; total time=   9.7s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=350, warm_start=True; total time=   8.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   7.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   6.9s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   7.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "140 fits failed out of a total of 210.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "112 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.46613595 0.46591822 0.4652321  0.46652565        nan 0.46582755\n",
      "        nan        nan        nan        nan 0.46570655        nan\n",
      "        nan        nan        nan 0.46539404        nan        nan\n",
      "        nan        nan 0.46523227        nan        nan        nan\n",
      "        nan        nan 0.46468418        nan        nan 0.46638002]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters for Random Forest:\n",
      " {'warm_start': True, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': True}\n",
      "Final R Score: 0.4414\n",
      "Final MSE: 91.0553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Select features and target\n",
    "df_selected_features = df_transformed[['outcome', 'depth', 'b3', 'b1', 'a1', 'a4', 'a3', 'a2', 'b5', 'a6', 'b10', 'b2',\n",
    "       'b9', 'a7', 'b6', 'a10', 'log_price']]\n",
    "\n",
    "target_col = \"outcome\"  \n",
    "X = df_selected_features.drop(columns=[target_col])  \n",
    "y = df_selected_features[target_col]  \n",
    "\n",
    "# Train-Test Split (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a base RandomForest model to check feature importance\n",
    "base_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "base_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances and drop low-importance features\n",
    "importances = base_rf.feature_importances_\n",
    "important_features = [X.columns[i] for i in range(len(importances)) if importances[i] > 0.01]  # Threshold to remove weak features\n",
    "X_train = X_train[important_features]\n",
    "X_val = X_val[important_features]\n",
    "\n",
    "# Expanded hyperparameter search space\n",
    "param_dist = {\n",
    "    \"n_estimators\": [350, 400, 450, 500],  # More trees for better accuracy\n",
    "    \"max_depth\": [20, 25, 30, None],  # Allow deeper trees\n",
    "    \"min_samples_split\": [2, 3, 5],  # Allow more splits\n",
    "    \"min_samples_leaf\": [1, 2],  # Allow smaller leaf sizes\n",
    "    \"max_features\": [\"auto\", \"sqrt\"],  # Allow more flexibility\n",
    "    \"bootstrap\": [True],\n",
    "    \"warm_start\": [True]  # Allows incremental tree growth\n",
    "}\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV with more iterations and better validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=30,  # Keep it high for better tuning\n",
    "    cv=7,  # More cross-validation folds for stability\n",
    "    scoring='r2', \n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42,\n",
    "    verbose=2  # Show detailed progress\n",
    ")\n",
    "\n",
    "# Fit hyperparameter search on training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the search\n",
    "best_params = random_search.best_params_\n",
    "final_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate model on validation set\n",
    "y_pred_final = final_model.predict(X_val)\n",
    "final_r2 = r2_score(y_val, y_pred_final)\n",
    "final_mse = mean_squared_error(y_val, y_pred_final)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest Hyperparameters for Random Forest:\\n\", best_params)\n",
    "print(f\"Final R Score: {final_r2:.4f}\")\n",
    "print(f\"Final MSE: {final_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65156032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  12.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  12.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  12.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=600, warm_start=True; total time=  15.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=600, warm_start=True; total time=  15.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=600, warm_start=True; total time=  16.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=600, warm_start=True; total time=  16.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=600, warm_start=True; total time=  16.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  12.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  12.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500, warm_start=True; total time=  13.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500, warm_start=True; total time=  13.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500, warm_start=True; total time=  13.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500, warm_start=True; total time=  13.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500, warm_start=True; total time=  13.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  15.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  15.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  15.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  16.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  15.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=1, min_samples_split=2, n_estimators=600, warm_start=True; total time=  42.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=1, min_samples_split=2, n_estimators=600, warm_start=True; total time=  43.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=  32.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=1, min_samples_split=2, n_estimators=600, warm_start=True; total time=  43.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=  31.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=1, min_samples_split=2, n_estimators=600, warm_start=True; total time=  42.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=  32.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=1, min_samples_split=2, n_estimators=600, warm_start=True; total time=  42.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=0.75, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=  23.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=0.75, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=  22.7s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=0.75, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=  23.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=0.75, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=  23.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=0.75, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=  24.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   9.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=  29.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=  29.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  12.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  12.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  12.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  12.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  12.7s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  12.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  13.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  13.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  12.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=600, warm_start=True; total time=  12.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=   8.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=  11.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=  10.9s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=  11.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=  10.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500, warm_start=True; total time=  10.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=   9.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=  10.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=   9.9s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=  10.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400, warm_start=True; total time=   9.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=   8.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=600, warm_start=True; total time=  12.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=600, warm_start=True; total time=  13.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=600, warm_start=True; total time=  12.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=600, warm_start=True; total time=  13.3s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=   8.7s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=600, warm_start=True; total time=  13.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=   8.5s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=   8.6s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=   8.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=600, warm_start=True; total time=  12.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=600, warm_start=True; total time=  13.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=600, warm_start=True; total time=  13.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=  21.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=  21.7s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=  22.4s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=  21.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=0.75, min_samples_leaf=2, min_samples_split=5, n_estimators=400, warm_start=True; total time=  22.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=600, warm_start=True; total time=  13.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500, warm_start=True; total time=  11.6s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=600, warm_start=True; total time=  13.3s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500, warm_start=True; total time=  11.6s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500, warm_start=True; total time=  11.8s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500, warm_start=True; total time=  11.5s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500, warm_start=True; total time=  11.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=   8.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=   9.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=   9.2s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=   8.5s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=0.75, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  26.8s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400, warm_start=True; total time=   8.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=0.75, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  26.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=0.75, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  25.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=0.75, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  25.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=0.75, min_samples_leaf=2, min_samples_split=5, n_estimators=500, warm_start=True; total time=  26.1s\n",
      "\n",
      "Best Hyperparameters for Random Forest:\n",
      " {'warm_start': True, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': True}\n",
      "Final R Score: 0.4423\n",
      "Final MSE: 90.9117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Only use the selected features\n",
    "selected_features = ['outcome', 'depth', 'b3', 'b1', 'a1', 'a4', 'a3', 'a2', 'b5', 'a6', 'b10', 'b2',\n",
    "       'b9', 'a7', 'b6', 'a10', 'log_price']\n",
    "\n",
    "# Select features and target\n",
    "df_selected_features = df_transformed[['outcome'] + selected_features]\n",
    "\n",
    "target_col = \"outcome\"  \n",
    "X = df_selected_features.drop(columns=[target_col])  \n",
    "y = df_selected_features[target_col]  \n",
    "\n",
    "# Train-Test Split (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# New optimized hyperparameter search space\n",
    "param_dist = {\n",
    "    \"n_estimators\": [400, 500, 600],  # Increased trees for stability\n",
    "    \"max_depth\": [20, 30, None],  # Deeper trees allow more learning\n",
    "    \"min_samples_split\": [2, 5],  # Fine-tuned for balance\n",
    "    \"min_samples_leaf\": [1, 2],  # Allow smaller leaves\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.75],  # Allow auto selection of features\n",
    "    \"bootstrap\": [True],  # Keep bootstrap enabled\n",
    "    \"warm_start\": [True]  # Incremental tree training for efficiency\n",
    "}\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV with optimized settings\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=20,  # More iterations for better tuning\n",
    "    cv=5,  # More folds for stability\n",
    "    scoring='r2', \n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42,\n",
    "    verbose=2  # Show detailed progress\n",
    ")\n",
    "\n",
    "# Fit hyperparameter search on training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the search\n",
    "best_params = random_search.best_params_\n",
    "final_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate model on validation set\n",
    "y_pred_final = final_model.predict(X_val)\n",
    "final_r2 = r2_score(y_val, y_pred_final)\n",
    "final_mse = mean_squared_error(y_val, y_pred_final)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest Hyperparameters for Random Forest:\\n\", best_params)\n",
    "print(f\"Final R Score: {final_r2:.4f}\")\n",
    "print(f\"Final MSE: {final_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7aad95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.1, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=1.0; total time=   1.2s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.1, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=1.0; total time=   1.2s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.1, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=1.0; total time=   1.2s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.2s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.2s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.3s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.3s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.1, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=1.0; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.1, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=1.0; total time=   1.2s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.01, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.01, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.01, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.01, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.01, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=   2.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.5s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0, colsample_bytree=0.6, gamma=0.1, lambda=1.2, learning_rate=0.1, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=0.6, gamma=0.1, lambda=1.2, learning_rate=0.1, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=0.6, gamma=0.1, lambda=1.2, learning_rate=0.1, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=0.6, gamma=0.1, lambda=1.2, learning_rate=0.1, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=0.6, gamma=0.1, lambda=1.2, learning_rate=0.1, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.1, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.1, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.1, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.6, gamma=0, lambda=0.8, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=0.8; total time=   2.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.6, gamma=0, lambda=0.8, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=0.8; total time=   2.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.6, gamma=0, lambda=0.8, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=0.8; total time=   2.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.1, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.2s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.6, gamma=0, lambda=0.8, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=0.8; total time=   2.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.1, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.6, gamma=0, lambda=0.8, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=0.8; total time=   2.7s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0.3, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0.3, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0.3, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   3.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   3.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   3.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   3.0s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0.1, lambda=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0.3, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0.3, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   2.8s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0, lambda=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0, lambda=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0, lambda=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0.1, lambda=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0.1, lambda=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0.1, lambda=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0, lambda=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0, lambda=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.8, gamma=0.1, lambda=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   1.1s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0, colsample_bytree=0.6, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.8s\n",
      "[CV] END alpha=0, colsample_bytree=0.6, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.8s\n",
      "[CV] END alpha=0, colsample_bytree=0.6, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.8s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.8s\n",
      "[CV] END alpha=0, colsample_bytree=0.6, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.8s\n",
      "[CV] END alpha=0, colsample_bytree=0.6, gamma=0, lambda=1.0, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.9s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.9s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.8s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.8s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=0.8, learning_rate=0.05, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.6, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.6, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.6, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.6, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.6, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "\n",
      "Best Hyperparameters for XGBoost:\n",
      " {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05, 'lambda': 1.2, 'gamma': 0.1, 'colsample_bytree': 1.0, 'alpha': 0.5}\n",
      "Final R Score (XGBoost): 0.4586\n",
      "Final MSE (XGBoost): 88.2566\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Only use selected features\n",
    "selected_features = ['depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4',\n",
    "       'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10',\n",
    "       'cut_Good', 'cut_Ideal', 'cut_Premium', 'cut_Very Good', 'color_E',\n",
    "       'color_F', 'color_G', 'color_H', 'color_I', 'color_J', 'clarity_SI1',\n",
    "       'clarity_SI2', 'clarity_VS1', 'clarity_VS2', 'clarity_VVS1',\n",
    "       'clarity_VVS2', 'volume', 'log_price', 'log_carat']\n",
    "\n",
    "df_selected_features = df_transformed[['outcome'] + selected_features]\n",
    "\n",
    "# Define target and features\n",
    "target_col = \"outcome\"\n",
    "X = df_selected_features.drop(columns=[target_col])\n",
    "y = df_selected_features[target_col]\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define XGBoost hyperparameter grid\n",
    "xgb_param_dist = {\n",
    "    \"n_estimators\": [200, 300, 400],  # More boosting rounds\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],  # Smaller rates improve convergence\n",
    "    \"max_depth\": [3, 5, 7],  # Controls tree complexity\n",
    "    \"subsample\": [0.6, 0.8, 1.0],  # Sample fraction for boosting\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],  # Feature selection per tree\n",
    "    \"gamma\": [0, 0.1, 0.3],  # Minimum loss reduction for split\n",
    "    \"lambda\": [0.8, 1.0, 1.2],  # L2 regularization\n",
    "    \"alpha\": [0, 0.1, 0.5]  # L1 regularization\n",
    "}\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with RandomizedSearchCV\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=xgb_param_dist,\n",
    "    n_iter=20,  # Balance speed vs accuracy\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Train model with hyperparameter tuning\n",
    "xgb_random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model and evaluation\n",
    "best_xgb_params = xgb_random_search.best_params_\n",
    "final_xgb_model = xgb_random_search.best_estimator_\n",
    "\n",
    "# Evaluate XGBoost on validation set\n",
    "y_pred_xgb = final_xgb_model.predict(X_val)\n",
    "final_r2_xgb = r2_score(y_val, y_pred_xgb)\n",
    "final_mse_xgb = mean_squared_error(y_val, y_pred_xgb)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest Hyperparameters for XGBoost:\\n\", best_xgb_params)\n",
    "print(f\"Final R Score (XGBoost): {final_r2_xgb:.4f}\")\n",
    "print(f\"Final MSE (XGBoost): {final_mse_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e38cf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 172127\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 823\n",
      "[LightGBM] [Info] Start training from score -4.957629\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Final R Score (Stacking Model): 0.4552\n",
      "Final MSE (Stacking Model): 88.7998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  1. Use Only Selected Features\n",
    "selected_features = ['depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4',\n",
    "       'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10',\n",
    "       'cut_Good', 'cut_Ideal', 'cut_Premium', 'cut_Very Good', 'color_E',\n",
    "       'color_F', 'color_G', 'color_H', 'color_I', 'color_J', 'clarity_SI1',\n",
    "       'clarity_SI2', 'clarity_VS1', 'clarity_VS2', 'clarity_VVS1',\n",
    "       'clarity_VVS2', 'volume', 'log_price', 'log_carat']\n",
    "\n",
    "df_selected_features = df_transformed[['outcome'] + selected_features]\n",
    "\n",
    "#  2. Define Features and Target\n",
    "target_col = \"outcome\"\n",
    "X = df_selected_features.drop(columns=[target_col])\n",
    "y = df_selected_features[target_col]\n",
    "\n",
    "#  3. Train-Test Split (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#  4. Feature Engineering: Polynomial Features (Degree 2)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_val_poly = poly.transform(X_val)\n",
    "\n",
    "#  5. Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_val_scaled = scaler.transform(X_val_poly)\n",
    "\n",
    "#  6. Define Models\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=300, learning_rate=0.03, max_depth=4, random_state=42)\n",
    "lgbm_model = lgb.LGBMRegressor(n_estimators=300, learning_rate=0.03, max_depth=4, random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=300, max_depth=20, random_state=42)\n",
    "ridge_model = Ridge(alpha=1.0)  # Ridge regression as meta-learner\n",
    "\n",
    "#  7. Train Base Models\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "lgbm_model.fit(X_train_scaled, y_train)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#  8. Get Predictions from Base Models\n",
    "xgb_preds = xgb_model.predict(X_val_scaled)\n",
    "lgbm_preds = lgbm_model.predict(X_val_scaled)\n",
    "rf_preds = rf_model.predict(X_val_scaled)\n",
    "\n",
    "#  9. Create Meta Model (Stacking)\n",
    "meta_X_train = np.column_stack((xgb_preds, lgbm_preds, rf_preds))\n",
    "ridge_model.fit(meta_X_train, y_val)\n",
    "\n",
    "#  10. Final Predictions\n",
    "final_preds = ridge_model.predict(meta_X_train)\n",
    "\n",
    "#  11. Evaluate Stacking Model\n",
    "final_r2 = r2_score(y_val, final_preds)\n",
    "final_mse = mean_squared_error(y_val, final_preds)\n",
    "\n",
    "#  12. Print Results\n",
    "print(f\"Final R Score (Stacking Model): {final_r2:.4f}\")\n",
    "print(f\"Final MSE (Stacking Model): {final_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53006967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 25 candidates, totalling 175 fits\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.025, max_depth=4, n_estimators=400, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.025, max_depth=4, n_estimators=400, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.025, max_depth=4, n_estimators=400, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.025, max_depth=4, n_estimators=400, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.025, max_depth=4, n_estimators=400, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.025, max_depth=4, n_estimators=400, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.025, max_depth=4, n_estimators=400, subsample=1.0; total time=   0.7s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.03, max_depth=6, n_estimators=250, subsample=0.7; total time=   1.1s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.03, max_depth=6, n_estimators=250, subsample=0.7; total time=   1.1s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.03, max_depth=6, n_estimators=250, subsample=0.7; total time=   1.1s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.03, max_depth=6, n_estimators=250, subsample=0.7; total time=   1.1s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.03, max_depth=6, n_estimators=250, subsample=0.7; total time=   1.0s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.03, max_depth=6, n_estimators=250, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.03, max_depth=6, n_estimators=250, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=400, subsample=0.7; total time=   0.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=400, subsample=0.7; total time=   0.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=450, subsample=0.9; total time=   2.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=450, subsample=0.9; total time=   2.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=400, subsample=0.7; total time=   0.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=400, subsample=0.7; total time=   0.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=400, subsample=0.7; total time=   0.6s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=400, subsample=0.7; total time=   0.6s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=450, subsample=0.9; total time=   2.2s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=400, subsample=0.7; total time=   0.6s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=450, subsample=0.9; total time=   2.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=450, subsample=0.9; total time=   2.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=450, subsample=0.9; total time=   2.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=450, subsample=0.9; total time=   2.4s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0.3, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.7; total time=   0.5s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0.3, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.7; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0.3, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.7; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0.3, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.7; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0.3, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.7; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0.3, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.7; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=1.0, gamma=0.3, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.7; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=300, subsample=0.8; total time=   0.5s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=300, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=300, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=300, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=300, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=300, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=300, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.1s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.1s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.1s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.1s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.0s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.1s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.0s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0.2, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=250, subsample=0.9; total time=   0.8s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=400, subsample=0.9; total time=   1.9s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0.2, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=250, subsample=0.9; total time=   0.8s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0.2, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=250, subsample=0.9; total time=   0.8s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0.2, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=250, subsample=0.9; total time=   0.8s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0.2, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=250, subsample=0.9; total time=   0.8s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0.2, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=250, subsample=0.9; total time=   0.8s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0.2, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=250, subsample=0.9; total time=   0.8s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.5, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=1.0; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.5, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=1.0; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.5, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=1.0; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.5, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=1.0; total time=   1.0s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.01, max_depth=4, n_estimators=250, subsample=1.0; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.01, max_depth=4, n_estimators=250, subsample=1.0; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.01, max_depth=4, n_estimators=250, subsample=1.0; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.01, max_depth=4, n_estimators=250, subsample=1.0; total time=   0.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.5, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=1.0; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.5, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=1.0; total time=   1.1s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.01, max_depth=4, n_estimators=250, subsample=1.0; total time=   0.5s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.01, max_depth=4, n_estimators=250, subsample=1.0; total time=   0.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=1.0, gamma=0.1, lambda=1.5, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=1.0; total time=   1.0s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.01, max_depth=4, n_estimators=250, subsample=1.0; total time=   0.5s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.1, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.2, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.2, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.2, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.2, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.0s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.1, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.2, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.2, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.2, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.9; total time=   1.0s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.1, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.1, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.1, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.1, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.3s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.1, lambda=1.5, learning_rate=0.025, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=400, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.8; total time=   0.9s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.8; total time=   1.0s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.8; total time=   1.0s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.8; total time=   1.0s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.8; total time=   0.9s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.8; total time=   0.9s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=5, n_estimators=350, subsample=0.8; total time=   1.0s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=400, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=400, subsample=0.8; total time=   0.7s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=400, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=400, subsample=0.8; total time=   0.7s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=400, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=350, subsample=1.0; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.8, gamma=0.2, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=400, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=350, subsample=1.0; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=350, subsample=1.0; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=350, subsample=1.0; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=350, subsample=1.0; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=350, subsample=1.0; total time=   0.6s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.1, lambda=1.2, learning_rate=0.03, max_depth=4, n_estimators=350, subsample=1.0; total time=   0.6s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=1.0; total time=   1.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=1.0; total time=   1.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=1.0; total time=   1.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=1.0; total time=   1.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=1.0; total time=   1.5s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=1.0; total time=   1.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.8, gamma=0.3, lambda=1.2, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=1.0; total time=   1.4s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=6, n_estimators=350, subsample=0.7; total time=   1.8s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.2, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=0.7; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=6, n_estimators=350, subsample=0.7; total time=   1.7s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=6, n_estimators=350, subsample=0.7; total time=   1.8s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=6, n_estimators=350, subsample=0.7; total time=   1.8s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.2, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=0.7; total time=   1.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=6, n_estimators=350, subsample=0.7; total time=   1.7s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=6, n_estimators=350, subsample=0.7; total time=   1.7s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0, lambda=1.5, learning_rate=0.025, max_depth=6, n_estimators=350, subsample=0.7; total time=   1.8s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=350, subsample=0.8; total time=   0.5s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.2, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=350, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.2, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=0.7; total time=   1.1s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=350, subsample=0.8; total time=   0.6s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.2, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.2, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=0.7; total time=   1.1s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=350, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=0.9, gamma=0.2, lambda=1.0, learning_rate=0.05, max_depth=5, n_estimators=350, subsample=0.7; total time=   1.2s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=350, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=350, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0.1, colsample_bytree=1.0, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=3, n_estimators=350, subsample=0.8; total time=   0.4s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0.1, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.3s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0.1, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.3s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0.1, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.3s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0.1, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.2s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0.1, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.2s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0.1, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.2s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0.1, lambda=1.5, learning_rate=0.03, max_depth=5, n_estimators=400, subsample=0.8; total time=   1.3s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=250, subsample=0.7; total time=   0.7s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=6, n_estimators=450, subsample=0.7; total time=   2.2s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=250, subsample=0.7; total time=   0.8s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=250, subsample=0.7; total time=   0.8s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=6, n_estimators=450, subsample=0.7; total time=   2.1s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=6, n_estimators=450, subsample=0.7; total time=   2.2s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=6, n_estimators=450, subsample=0.7; total time=   2.0s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=6, n_estimators=450, subsample=0.7; total time=   2.2s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=6, n_estimators=450, subsample=0.7; total time=   2.1s\n",
      "[CV] END alpha=0, colsample_bytree=0.9, gamma=0.2, lambda=1.2, learning_rate=0.05, max_depth=6, n_estimators=450, subsample=0.7; total time=   2.1s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=250, subsample=0.7; total time=   0.8s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=250, subsample=0.7; total time=   0.7s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=250, subsample=0.7; total time=   0.8s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.9, gamma=0, lambda=1.2, learning_rate=0.05, max_depth=5, n_estimators=250, subsample=0.7; total time=   0.9s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.3, lambda=1.0, learning_rate=0.025, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.3, lambda=1.0, learning_rate=0.025, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.1s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.3, lambda=1.0, learning_rate=0.025, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=0.9; total time=   1.3s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.3, lambda=1.0, learning_rate=0.025, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.3, lambda=1.0, learning_rate=0.025, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.2s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.3, lambda=1.0, learning_rate=0.025, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.5, colsample_bytree=0.9, gamma=0.3, lambda=1.0, learning_rate=0.025, max_depth=6, n_estimators=400, subsample=0.9; total time=   2.0s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=0.9; total time=   1.2s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=0.9; total time=   1.2s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=0.9; total time=   1.2s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=0.9; total time=   1.2s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=0.9; total time=   1.2s\n",
      "[CV] END alpha=0.3, colsample_bytree=0.8, gamma=0.2, lambda=1.5, learning_rate=0.01, max_depth=5, n_estimators=450, subsample=0.9; total time=   1.1s\n",
      "\n",
      "Final Optimized XGBoost Hyperparameters:\n",
      " {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.03, 'lambda': 1.2, 'gamma': 0.2, 'colsample_bytree': 0.9, 'alpha': 0}\n",
      "Final Optimized R Score (XGBoost): 0.4567\n",
      "Final Optimized MSE (XGBoost): 88.5557\n"
     ]
    }
   ],
   "source": [
    "todrop = ['cut_Ideal', 'color_F', 'cut_Very Good', 'clarity_IF', 'cut_Premium', 'clarity_SI2', 'clarity_VS2', 'clarity_VVS1']\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# #  Step 1: Use only the most important features\n",
    "# selected_features = ['depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4',\n",
    "#        'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10',\n",
    "#        'cut_Good', 'cut_Ideal', 'cut_Premium', 'cut_Very Good', 'color_E',\n",
    "#        'color_F', 'color_G', 'color_H', 'color_I', 'color_J', 'clarity_SI1',\n",
    "#        'clarity_SI2', 'clarity_VS1', 'clarity_VS2', 'clarity_VVS1',\n",
    "#        'clarity_VVS2', 'volume', 'log_price', 'log_carat']\n",
    "selected_features = selected_features = ['depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4',\n",
    "       'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10',\n",
    "       'cut_Good', 'color_E', 'color_G', 'color_H', 'color_I', 'color_J', \n",
    "       'clarity_SI1', 'clarity_VS1', 'clarity_VVS2', 'volume', 'log_price', 'log_carat']\n",
    "\n",
    "df_selected_features = df_transformed[['outcome'] + selected_features]\n",
    "\n",
    "#  Step 2: Define Features and Target\n",
    "target_col = \"outcome\"\n",
    "X = df_selected_features.drop(columns=[target_col])\n",
    "y = df_selected_features[target_col]\n",
    "\n",
    "#  Step 3: Train-Test Split (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#  Step 4: XGBoost Hyperparameter Search with More Granularity\n",
    "xgb_param_dist = {\n",
    "    \"n_estimators\": [250, 300, 350, 400, 450],  # More trees to refine learning\n",
    "    \"learning_rate\": [0.01, 0.025, 0.03, 0.05],  # Slower learning for stability\n",
    "    \"max_depth\": [3, 4, 5, 6],  # Slightly deeper trees\n",
    "    \"subsample\": [0.7, 0.8, 0.9, 1.0],  # Controlling sample diversity\n",
    "    \"colsample_bytree\": [0.8, 0.9, 1.0],  # Feature selection per tree\n",
    "    \"gamma\": [0, 0.1, 0.2, 0.3],  # Pruning weak splits\n",
    "    \"lambda\": [1.0, 1.2, 1.5],  # L2 regularization\n",
    "    \"alpha\": [0, 0.1, 0.3, 0.5]  # L1 regularization\n",
    "}\n",
    "\n",
    "#  Step 5: Initialize XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "\n",
    "#  Step 6: Hyperparameter tuning with RandomizedSearchCV (Increased CV to 7)\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=xgb_param_dist,\n",
    "    n_iter=25,  # More trials for better tuning\n",
    "    cv=7,  # More folds for stability\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "#  Step 7: Train Model with Best Parameters\n",
    "xgb_random_search.fit(X_train, y_train)\n",
    "\n",
    "#  Step 8: Best Model Selection\n",
    "best_xgb_params = xgb_random_search.best_params_\n",
    "final_xgb_model = xgb_random_search.best_estimator_\n",
    "\n",
    "#  Step 9: Evaluate Final Model on Validation Set\n",
    "y_pred_xgb = final_xgb_model.predict(X_val)\n",
    "final_r2_xgb = r2_score(y_val, y_pred_xgb)\n",
    "final_mse_xgb = mean_squared_error(y_val, y_pred_xgb)\n",
    "\n",
    "#  Step 10: Print Results\n",
    "print(\"\\nFinal Optimized XGBoost Hyperparameters:\\n\", best_xgb_params)\n",
    "print(f\"Final Optimized R Score (XGBoost): {final_r2_xgb:.4f}\")\n",
    "print(f\"Final Optimized MSE (XGBoost): {final_mse_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c8bc13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 12.5592296\ttotal: 57.9ms\tremaining: 17.3s\n",
      "100:\tlearn: 9.2763525\ttotal: 164ms\tremaining: 323ms\n",
      "200:\tlearn: 9.0578158\ttotal: 265ms\tremaining: 130ms\n",
      "299:\tlearn: 8.9270651\ttotal: 366ms\tremaining: 0us\n",
      "Final R Score (CatBoost): 0.4608\n",
      "Final MSE (CatBoost): 87.8991\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "#  Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#  Define CatBoost Model\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=300, \n",
    "    learning_rate=0.03, \n",
    "    depth=4, \n",
    "    l2_leaf_reg=1.2,\n",
    "    loss_function='RMSE',\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "#  Train Model\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "#  Evaluate CatBoost on Validation Set\n",
    "y_pred_cat = cat_model.predict(X_val)\n",
    "final_r2_cat = r2_score(y_val, y_pred_cat)\n",
    "final_mse_cat = mean_squared_error(y_val, y_pred_cat)\n",
    "\n",
    "#  Print Results\n",
    "print(f\"Final R Score (CatBoost): {final_r2_cat:.4f}\")\n",
    "print(f\"Final MSE (CatBoost): {final_mse_cat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfc9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 30 candidates, totalling 210 fits\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   4.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.4s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   8.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.6s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   4.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   4.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   4.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   4.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   4.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   4.5s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   6.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   2.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   2.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   2.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   6.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   7.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   7.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   7.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   7.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   7.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   2.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   2.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   3.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   3.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.4s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   5.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   5.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   5.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   5.1s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.4s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   4.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   5.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   5.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   6.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   6.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   6.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   5.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   5.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   5.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "112 fits failed out of a total of 210.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "84 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 5807, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 2381, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 2307, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6382, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6404, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: /Users/zomb-ml-platform-msk/go-agent-21.2.0/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/bootstrap_options.cpp:16: Error: bayesian bootstrap doesn't support 'subsample' option\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 5807, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 2396, in _fit\n",
      "    self._train(\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 1776, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4833, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4882, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: /Users/zomb-ml-platform-msk/go-agent-21.2.0/pipelines/BuildMaster/catboost.git/catboost/libs/metrics/metric.cpp:4939: Metric Huber requires delta as parameter\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.4715768  0.47015779        nan 0.47574475 0.47325264 0.46480892\n",
      "        nan        nan 0.4789742  0.47437554        nan 0.47538271\n",
      " 0.47314264 0.47342174        nan        nan        nan        nan\n",
      "        nan        nan 0.47024165        nan        nan        nan\n",
      "        nan 0.48015043        nan 0.47965447 0.47618431        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Optimized CatBoost Hyperparameters:\n",
      " {'subsample': 0.8, 'loss_function': 'RMSE', 'learning_rate': 0.02, 'l2_leaf_reg': 3, 'iterations': 700, 'depth': 5, 'colsample_bylevel': 1.0, 'bootstrap_type': 'Bernoulli'}\n",
      " Final Optimized R Score (CatBoost): 0.4641\n",
      " Final Optimized MSE (CatBoost): 87.3564\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "selected_features = selected_features = ['depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4',\n",
    "       'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10',\n",
    "       'cut_Good', 'color_E', 'color_G', 'color_H', 'color_I', 'color_J', \n",
    "       'clarity_SI1', 'clarity_VS1', 'clarity_VVS2', 'volume', 'log_price', 'log_carat']\n",
    "\n",
    "df_selected_features = df_transformed[['outcome'] + selected_features]\n",
    "\n",
    "#  Step 2: Define Features and Target\n",
    "target_col = \"outcome\"\n",
    "X = df_selected_features.drop(columns=[target_col])\n",
    "y = df_selected_features[target_col]\n",
    "\n",
    "#  Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#  Define Hyperparameter Grid for Tuning\n",
    "cat_param_dist = {\n",
    "    \"iterations\": [500, 700, 1000],  # More boosting rounds\n",
    "    \"learning_rate\": [0.01, 0.02, 0.03],  # Slower for stability\n",
    "    \"depth\": [4, 5, 6, 7],  # Test deeper trees\n",
    "    \"l2_leaf_reg\": [1, 3, 5, 7],  # Regularization tuning\n",
    "    \"loss_function\": ['RMSE', 'Huber', 'Quantile'],  # Robust loss functions\n",
    "    \"colsample_bylevel\": [0.8, 0.9, 1.0],  # Feature selection per level\n",
    "    \"subsample\": [0.7, 0.8, 0.9],  # Sample diversity control\n",
    "    \"bootstrap_type\": ['Bayesian', 'Bernoulli'],  # Better handling of bootstrapping\n",
    "}\n",
    "\n",
    "#  Initialize CatBoost Model (No Verbose for Faster Training)\n",
    "cat_model = CatBoostRegressor(random_seed=42, verbose=0)\n",
    "\n",
    "#  Perform Hyperparameter Tuning\n",
    "cat_random_search = RandomizedSearchCV(\n",
    "    cat_model,\n",
    "    param_distributions=cat_param_dist,\n",
    "    n_iter=30,  # More iterations for better tuning\n",
    "    cv=7,  # More folds for stability\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "#  Train Model with Best Parameters\n",
    "cat_random_search.fit(X_train, y_train)\n",
    "\n",
    "#  Get Best Model\n",
    "best_cat_params = cat_random_search.best_params_\n",
    "final_cat_model = cat_random_search.best_estimator_\n",
    "\n",
    "#  Evaluate on Validation Set\n",
    "y_pred_cat = final_cat_model.predict(X_val)\n",
    "final_r2_cat = r2_score(y_val, y_pred_cat)\n",
    "final_mse_cat = mean_squared_error(y_val, y_pred_cat)\n",
    "\n",
    "#  Print Results\n",
    "print(\"\\n Final Optimized CatBoost Hyperparameters:\\n\", best_cat_params)\n",
    "print(f\" Final Optimized R Score (CatBoost): {final_r2_cat:.4f}\")\n",
    "print(f\" Final Optimized MSE (CatBoost): {final_mse_cat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63baa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf0f8161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 20 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   7.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   8.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   8.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   8.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   8.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   8.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   8.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   9.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   8.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   7.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   7.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   8.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   8.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   8.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile; total time=   9.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile; total time=   9.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=Quantile; total time=   3.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=Quantile; total time=   3.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=Quantile; total time=   3.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=Quantile; total time=   3.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile; total time=   9.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=Quantile; total time=   3.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile; total time=   8.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=Quantile; total time=   3.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile; total time=   9.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile; total time=   9.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile; total time=   9.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=Quantile; total time=   3.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   3.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   3.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   3.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   3.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   3.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   3.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   3.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   8.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=Quantile; total time=   5.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   8.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   8.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   9.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   9.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   9.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=800, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=   8.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=Quantile; total time=   5.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=Quantile; total time=   5.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   4.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=Quantile; total time=   5.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   4.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=Quantile; total time=   5.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=Quantile; total time=   5.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=Quantile; total time=   5.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   4.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   4.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   3.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   3.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   3.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=   7.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=   7.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=   7.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=   7.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=   7.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=   7.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=   8.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=  11.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile; total time=   4.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile; total time=   3.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=  11.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=  10.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=  11.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile; total time=   3.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=  10.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=  10.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.01, loss_function=RMSE; total time=  10.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile; total time=   3.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile; total time=   3.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile; total time=   3.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=RMSE; total time=   3.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile; total time=   3.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=RMSE; total time=   3.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=RMSE; total time=   3.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=RMSE; total time=   3.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=RMSE; total time=   3.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=RMSE; total time=   3.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.015, loss_function=RMSE; total time=   3.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE; total time=   4.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE; total time=   5.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE; total time=   4.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE; total time=   4.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE; total time=   4.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE; total time=   5.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE; total time=   5.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   7.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   4.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   8.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   7.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   7.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   8.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   4.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   7.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=RMSE; total time=   7.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   4.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   4.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   4.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   5.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=RMSE; total time=   5.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=   9.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=  10.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=  10.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=   9.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=  10.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=  10.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=7, learning_rate=0.015, loss_function=RMSE; total time=   9.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   9.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   9.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   9.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   9.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   8.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   9.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=800, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   9.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   8.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   8.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=Quantile; total time=   4.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=Quantile; total time=   4.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   7.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=Quantile; total time=   4.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   7.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   7.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   7.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.015, loss_function=RMSE; total time=   7.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=Quantile; total time=   4.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=Quantile; total time=   4.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=Quantile; total time=   4.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile; total time=   3.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=800, l2_leaf_reg=5, learning_rate=0.01, loss_function=Quantile; total time=   4.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile; total time=   3.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile; total time=   3.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile; total time=   3.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile; total time=   2.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile; total time=   2.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile; total time=   2.7s\n",
      "\n",
      " Final Optimized CatBoost Hyperparameters:\n",
      " {'loss_function': 'RMSE', 'learning_rate': 0.02, 'l2_leaf_reg': 7, 'iterations': 800, 'depth': 5, 'colsample_bylevel': 1.0, 'bootstrap_type': 'Bernoulli'}\n",
      " Final Optimized R Score (CatBoost): 0.4633\n",
      " Final Optimized MSE (CatBoost): 87.4890\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "#  Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#  Fix Hyperparameter Grid\n",
    "cat_param_dist = {\n",
    "    \"iterations\": [700, 800, 1000],  # More boosting rounds\n",
    "    \"learning_rate\": [0.01, 0.015, 0.02],  # Lower learning rate for stability\n",
    "    \"depth\": [5, 6, 7],  # Test slightly deeper trees\n",
    "    \"l2_leaf_reg\": [3, 5, 7],  # More tuning on L2 regularization\n",
    "    \"loss_function\": ['RMSE', 'Quantile'],  # Best performing loss functions\n",
    "    \"colsample_bylevel\": [0.9, 1.0],  # Feature selection per level\n",
    "    \"bootstrap_type\": ['Bernoulli'],  # Fix Bayesian error\n",
    "}\n",
    "\n",
    "#  Initialize CatBoost Model\n",
    "cat_model = CatBoostRegressor(random_seed=42, verbose=0)\n",
    "\n",
    "#  Perform Hyperparameter Tuning\n",
    "cat_random_search = RandomizedSearchCV(\n",
    "    cat_model,\n",
    "    param_distributions=cat_param_dist,\n",
    "    n_iter=20,  # Optimized for performance\n",
    "    cv=7,  # More folds for better generalization\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "#  Train Model with Best Parameters\n",
    "cat_random_search.fit(X_train, y_train)\n",
    "\n",
    "#  Get Best Model\n",
    "best_cat_params = cat_random_search.best_params_\n",
    "final_cat_model = cat_random_search.best_estimator_\n",
    "\n",
    "#  Evaluate on Validation Set\n",
    "y_pred_cat = final_cat_model.predict(X_val)\n",
    "final_r2_cat = r2_score(y_val, y_pred_cat)\n",
    "final_mse_cat = mean_squared_error(y_val, y_pred_cat)\n",
    "\n",
    "#  Print Results\n",
    "print(\"\\n Final Optimized CatBoost Hyperparameters:\\n\", best_cat_params)\n",
    "print(f\" Final Optimized R Score (CatBoost): {final_r2_cat:.4f}\")\n",
    "print(f\" Final Optimized MSE (CatBoost): {final_mse_cat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77b4dad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/z3ly6fls4z3ff2kwbph60rsh0000gn/T/ipykernel_47119/3272782103.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_features['price_per_volume'] = df_selected_features['log_price'] / df_selected_features['volume']\n",
      "/var/folders/l8/z3ly6fls4z3ff2kwbph60rsh0000gn/T/ipykernel_47119/3272782103.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_features['depth_ratio'] = df_selected_features['depth'] / df_selected_features['table']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 15.3868247\ttotal: 1.46ms\tremaining: 1.17s\n",
      "100:\tlearn: 10.3048734\ttotal: 123ms\tremaining: 850ms\n",
      "200:\tlearn: 9.9126673\ttotal: 253ms\tremaining: 754ms\n",
      "300:\tlearn: 9.7584795\ttotal: 377ms\tremaining: 625ms\n",
      "400:\tlearn: 9.6541523\ttotal: 500ms\tremaining: 497ms\n",
      "500:\tlearn: 9.5560794\ttotal: 646ms\tremaining: 385ms\n",
      "600:\tlearn: 9.4617479\ttotal: 780ms\tremaining: 258ms\n",
      "700:\tlearn: 9.3724802\ttotal: 920ms\tremaining: 130ms\n",
      "799:\tlearn: 9.2906986\ttotal: 1.04s\tremaining: 0us\n",
      "\n",
      " Final Attempt - Optimized CatBoost with Feature Engineering\n",
      " Final R Score (CatBoost): 0.4611\n",
      " Final MSE (CatBoost): 85.4543\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "selected_features = ['depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4',\n",
    "       'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10',\n",
    "       'cut_Good', 'cut_Ideal', 'cut_Premium', 'cut_Very Good', 'color_E',\n",
    "       'color_F', 'color_G', 'color_H', 'color_I', 'color_J', 'clarity_SI1',\n",
    "       'clarity_SI2', 'clarity_VS1', 'clarity_VS2', 'clarity_VVS1',\n",
    "       'clarity_VVS2', 'volume', 'log_price', 'log_carat']\n",
    "df_selected_features = df_transformed[['outcome'] + selected_features]\n",
    "\n",
    "#  Step 1: Create New Features\n",
    "df_selected_features['price_per_volume'] = df_selected_features['log_price'] / df_selected_features['volume']\n",
    "df_selected_features['depth_ratio'] = df_selected_features['depth'] / df_selected_features['table']\n",
    "\n",
    "#  Step 2: Define Features & Target\n",
    "target_col = \"outcome\"\n",
    "X = df_selected_features.drop(columns=[target_col])\n",
    "y = df_selected_features[target_col]\n",
    "\n",
    "#  Step 3: Train-Test Split (90-10 instead of 80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#  Step 4: Define Final Optimized CatBoost Model\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=800, \n",
    "    learning_rate=0.02, \n",
    "    depth=5, \n",
    "    l2_leaf_reg=7, \n",
    "    colsample_bylevel=1.0, \n",
    "    bootstrap_type=\"Bernoulli\",\n",
    "    loss_function=\"Huber:delta=1.5\",  # Huber Loss with Delta 1.5\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "#  Step 5: Train Model\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "#  Step 6: Evaluate on Validation Set\n",
    "y_pred_cat = cat_model.predict(X_val)\n",
    "final_r2_cat = r2_score(y_val, y_pred_cat)\n",
    "final_mse_cat = mean_squared_error(y_val, y_pred_cat)\n",
    "\n",
    "#  Step 7: Print Results\n",
    "print(f\"\\n Final Attempt - Optimized CatBoost with Feature Engineering\")\n",
    "print(f\" Final R Score (CatBoost): {final_r2_cat:.4f}\")\n",
    "print(f\" Final MSE (CatBoost): {final_mse_cat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9480485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/l8/z3ly6fls4z3ff2kwbph60rsh0000gn/T/ipykernel_47119/3788596918.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_features['price_per_volume'] = df_selected_features['log_price'] / df_selected_features['volume']\n",
      "/var/folders/l8/z3ly6fls4z3ff2kwbph60rsh0000gn/T/ipykernel_47119/3788596918.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_features['depth_ratio'] = df_selected_features['depth'] / df_selected_features['table']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 15.3937174\ttotal: 3.04ms\tremaining: 2.74s\n",
      "100:\tlearn: 10.3414431\ttotal: 198ms\tremaining: 1.56s\n",
      "200:\tlearn: 9.8602365\ttotal: 372ms\tremaining: 1.29s\n",
      "300:\tlearn: 9.6661042\ttotal: 556ms\tremaining: 1.11s\n",
      "400:\tlearn: 9.5295484\ttotal: 739ms\tremaining: 919ms\n",
      "500:\tlearn: 9.4098181\ttotal: 912ms\tremaining: 726ms\n",
      "600:\tlearn: 9.3025300\ttotal: 1.07s\tremaining: 535ms\n",
      "700:\tlearn: 9.1912791\ttotal: 1.24s\tremaining: 352ms\n",
      "800:\tlearn: 9.0920849\ttotal: 1.4s\tremaining: 173ms\n",
      "899:\tlearn: 8.9884900\ttotal: 1.56s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#  Display Feature Importance Table\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mace_tools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[1;32m     51\u001b[0m tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Importance\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mfeature_importance)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  Step 1: Use the current best feature set with engineered features\n",
    "df_selected_features['price_per_volume'] = df_selected_features['log_price'] / df_selected_features['volume']\n",
    "df_selected_features['depth_ratio'] = df_selected_features['depth'] / df_selected_features['table']\n",
    "\n",
    "#  Define target and feature matrix\n",
    "target_col = \"outcome\"\n",
    "X = df_selected_features.drop(columns=[target_col])\n",
    "y = df_selected_features[target_col]\n",
    "\n",
    "#  Train-Test Split (Keeping 90-10 for More Training Data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#  Train Best-Performing CatBoost Model\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=900,\n",
    "    learning_rate=0.018,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=5,\n",
    "    colsample_bylevel=0.9,\n",
    "    bootstrap_type=\"Bernoulli\",\n",
    "    loss_function=\"Huber:delta=1.5\",\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "#  Get Feature Importance from CatBoost\n",
    "feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': cat_model.get_feature_importance()})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "#  Display Feature Importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance from CatBoost\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "#  Display Feature Importance Table\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Feature Importance\", dataframe=feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6bfd1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Features After Selection: ['depth', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4', 'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'volume', 'log_price', 'depth_ratio']\n"
     ]
    }
   ],
   "source": [
    "#  Step 1: Remove Weak Features\n",
    "low_importance_features = feature_importance[feature_importance[\"Importance\"] < 0.5][\"Feature\"].tolist()\n",
    "X_filtered = X.drop(columns=low_importance_features)\n",
    "\n",
    "#  Step 2: Remove Highly Correlated Features\n",
    "correlation_matrix = X_filtered.corr().abs()\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "high_correlation_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)]\n",
    "X_filtered = X_filtered.drop(columns=high_correlation_features)\n",
    "\n",
    "#  Display Remaining Features After Selection\n",
    "print(f\"Remaining Features After Selection: {X_filtered.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8d2987e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 15.3982871\ttotal: 1.94ms\tremaining: 1.74s\n",
      "100:\tlearn: 10.3175700\ttotal: 149ms\tremaining: 1.18s\n",
      "200:\tlearn: 9.8579570\ttotal: 303ms\tremaining: 1.05s\n",
      "300:\tlearn: 9.6754052\ttotal: 451ms\tremaining: 897ms\n",
      "400:\tlearn: 9.5483415\ttotal: 603ms\tremaining: 751ms\n",
      "500:\tlearn: 9.4321593\ttotal: 777ms\tremaining: 619ms\n",
      "600:\tlearn: 9.3326703\ttotal: 934ms\tremaining: 465ms\n",
      "700:\tlearn: 9.2237087\ttotal: 1.09s\tremaining: 308ms\n",
      "800:\tlearn: 9.1231319\ttotal: 1.26s\tremaining: 155ms\n",
      "899:\tlearn: 9.0303348\ttotal: 1.4s\tremaining: 0us\n",
      "\n",
      " FINAL Best CatBoost Model After Feature Selection\n",
      " FINAL R Score (CatBoost): 0.4550\n",
      " FINAL MSE (CatBoost): 86.4168\n"
     ]
    }
   ],
   "source": [
    "#  Train-Test Split with New Feature Set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_filtered, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#  Train Final Optimized CatBoost Model\n",
    "final_cat_model = CatBoostRegressor(\n",
    "    iterations=900,\n",
    "    learning_rate=0.018,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=5,\n",
    "    colsample_bylevel=0.9,\n",
    "    bootstrap_type=\"Bernoulli\",\n",
    "    loss_function=\"Huber:delta=1.5\",\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "final_cat_model.fit(X_train, y_train)\n",
    "\n",
    "#  Evaluate on Validation Set\n",
    "y_pred_cat = final_cat_model.predict(X_val)\n",
    "final_r2_cat = r2_score(y_val, y_pred_cat)\n",
    "final_mse_cat = mean_squared_error(y_val, y_pred_cat)\n",
    "\n",
    "#  Print Final Results\n",
    "print(f\"\\n FINAL Best CatBoost Model After Feature Selection\")\n",
    "print(f\" FINAL R Score (CatBoost): {final_r2_cat:.4f}\")\n",
    "print(f\" FINAL MSE (CatBoost): {final_mse_cat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7dac1a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 12.6575468\ttotal: 2.35ms\tremaining: 2.35s\n",
      "100:\tlearn: 9.5440315\ttotal: 233ms\tremaining: 2.07s\n",
      "200:\tlearn: 9.0683946\ttotal: 462ms\tremaining: 1.84s\n",
      "300:\tlearn: 8.8635581\ttotal: 734ms\tremaining: 1.7s\n",
      "400:\tlearn: 8.7175276\ttotal: 1.05s\tremaining: 1.57s\n",
      "500:\tlearn: 8.5900915\ttotal: 1.3s\tremaining: 1.3s\n",
      "600:\tlearn: 8.4709482\ttotal: 1.53s\tremaining: 1.02s\n",
      "700:\tlearn: 8.3572150\ttotal: 1.75s\tremaining: 746ms\n",
      "800:\tlearn: 8.2406363\ttotal: 1.97s\tremaining: 490ms\n",
      "900:\tlearn: 8.1197806\ttotal: 2.21s\tremaining: 243ms\n",
      "999:\tlearn: 8.0089927\ttotal: 2.46s\tremaining: 0us\n",
      "\n",
      " FINAL Best CatBoost Model with Optimized Feature Selection\n",
      " FINAL R Score (CatBoost): 0.4541\n",
      " FINAL MSE (CatBoost): 86.5566\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "#  Step 1: Keep Only the Top 20 Features Based on Importance Scores\n",
    "top_20_features = feature_importance.iloc[:20][\"Feature\"].tolist()\n",
    "X_filtered = X[top_20_features]\n",
    "\n",
    "#  Step 2: Train-Test Split (Keeping 90-10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_filtered, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#  Step 3: Final Optimized CatBoost Model\n",
    "final_cat_model = CatBoostRegressor(\n",
    "    iterations=1000,  # More iterations for better convergence\n",
    "    learning_rate=0.015,  # Slow learning rate for stability\n",
    "    depth=7,  # Slightly deeper trees to capture interactions\n",
    "    l2_leaf_reg=5,  # Optimized regularization\n",
    "    colsample_bylevel=0.9,  # Reduce risk of overfitting\n",
    "    bootstrap_type=\"Bernoulli\",\n",
    "    loss_function=\"RMSE\",  # Switching back to RMSE for better fit\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "#  Step 4: Train Model\n",
    "final_cat_model.fit(X_train, y_train)\n",
    "\n",
    "#  Step 5: Evaluate Model on Validation Set\n",
    "y_pred_cat = final_cat_model.predict(X_val)\n",
    "final_r2_cat = r2_score(y_val, y_pred_cat)\n",
    "final_mse_cat = mean_squared_error(y_val, y_pred_cat)\n",
    "\n",
    "#  Step 6: Print Final Results\n",
    "print(f\"\\n FINAL Best CatBoost Model with Optimized Feature Selection\")\n",
    "print(f\" FINAL R Score (CatBoost): {final_r2_cat:.4f}\")\n",
    "print(f\" FINAL MSE (CatBoost): {final_mse_cat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4edff962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 12.6391958\ttotal: 2.61ms\tremaining: 2.08s\n",
      "100:\tlearn: 9.4699033\ttotal: 140ms\tremaining: 968ms\n",
      "200:\tlearn: 9.1673336\ttotal: 263ms\tremaining: 783ms\n",
      "300:\tlearn: 9.0359586\ttotal: 385ms\tremaining: 638ms\n",
      "400:\tlearn: 8.9444630\ttotal: 504ms\tremaining: 502ms\n",
      "500:\tlearn: 8.8627010\ttotal: 622ms\tremaining: 371ms\n",
      "600:\tlearn: 8.7748451\ttotal: 743ms\tremaining: 246ms\n",
      "700:\tlearn: 8.6938471\ttotal: 865ms\tremaining: 122ms\n",
      "799:\tlearn: 8.6108969\ttotal: 986ms\tremaining: 0us\n",
      "\n",
      " Final Optimized CatBoost Hyperparameters:\n",
      " {'loss_function': 'RMSE', 'learning_rate': 0.02, 'l2_leaf_reg': 7, 'iterations': 800, 'depth': 5, 'colsample_bylevel': 1.0, 'bootstrap_type': 'Bernoulli'}\n",
      " Final Optimized R Score (CatBoost): 0.4541\n",
      " Final Optimized MSE (CatBoost): 86.5566\n",
      " Best CatBoost model saved as: best_catboost_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "#  Load Data (Ensure X and y are defined from the best model's setup)\n",
    "# If needed, redefine the best feature set from previous runs\n",
    "# df_selected_features should be the dataset used in the best model run\n",
    "\n",
    "#  Train-Test Split (90-10 for More Training Data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#  Recreate the Best Model Configuration\n",
    "best_catboost_model = CatBoostRegressor(\n",
    "    iterations=800,\n",
    "    learning_rate=0.02,\n",
    "    depth=5,\n",
    "    l2_leaf_reg=7,\n",
    "    colsample_bylevel=1.0,\n",
    "    bootstrap_type=\"Bernoulli\",\n",
    "    loss_function=\"RMSE\",\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "#  Train the Best Model\n",
    "best_catboost_model.fit(X_train, y_train)\n",
    "\n",
    "#  Save the trained model\n",
    "#create a file path to save the model\n",
    "model_filename = \"best_catboost_model.pkl\"\n",
    "joblib.dump(best_catboost_model, model_filename)\n",
    "\n",
    "\n",
    "#  Evaluate on Validation Set\n",
    "y_pred_cat = final_cat_model.predict(X_val)\n",
    "final_r2_cat = r2_score(y_val, y_pred_cat)\n",
    "final_mse_cat = mean_squared_error(y_val, y_pred_cat)\n",
    "\n",
    "#  Print Results\n",
    "print(\"\\n Final Optimized CatBoost Hyperparameters:\\n\", best_cat_params)\n",
    "print(f\" Final Optimized R Score (CatBoost): {final_r2_cat:.4f}\")\n",
    "print(f\" Final Optimized MSE (CatBoost): {final_mse_cat:.4f}\")\n",
    "\n",
    "#  Confirm the model is saved\n",
    "print(f\" Best CatBoost model saved as: {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e9a6ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 30 candidates, totalling 210 fits\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   4.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   5.4s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   8.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   8.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   7.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.9; total time=   2.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.4s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.4s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=5, learning_rate=0.02, loss_function=Huber, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   1.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   4.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=1000, l2_leaf_reg=1, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.9s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.01, loss_function=Quantile, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   2.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   6.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   2.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   2.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   6.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   6.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   6.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   6.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   6.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.01, loss_function=RMSE, subsample=0.8; total time=   6.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   1.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.6s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=500, l2_leaf_reg=5, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=6, iterations=700, l2_leaf_reg=7, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=1000, l2_leaf_reg=3, learning_rate=0.02, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=1.0, depth=5, iterations=500, l2_leaf_reg=1, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=RMSE, subsample=0.9; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=5, iterations=700, l2_leaf_reg=7, learning_rate=0.03, loss_function=Quantile, subsample=0.8; total time=   2.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Quantile, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=7, learning_rate=0.03, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=5, iterations=500, l2_leaf_reg=5, learning_rate=0.02, loss_function=Quantile, subsample=0.8; total time=   2.2s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   4.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   4.9s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   4.7s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.8, depth=6, iterations=700, l2_leaf_reg=1, learning_rate=0.02, loss_function=Huber, subsample=0.8; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   4.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bayesian, colsample_bylevel=0.9, depth=7, iterations=700, l2_leaf_reg=5, learning_rate=0.01, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   4.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=4, iterations=700, l2_leaf_reg=1, learning_rate=0.03, loss_function=RMSE, subsample=0.8; total time=   2.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   4.8s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=7, iterations=500, l2_leaf_reg=1, learning_rate=0.03, loss_function=Quantile, subsample=0.7; total time=   4.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=1.0, depth=5, iterations=700, l2_leaf_reg=3, learning_rate=0.02, loss_function=RMSE, subsample=0.8; total time=   3.1s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.9, depth=7, iterations=1000, l2_leaf_reg=3, learning_rate=0.03, loss_function=Huber, subsample=0.7; total time=   0.0s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   6.4s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   6.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   6.3s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   5.7s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   5.5s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   5.2s\n",
      "[CV] END bootstrap_type=Bernoulli, colsample_bylevel=0.8, depth=7, iterations=700, l2_leaf_reg=7, learning_rate=0.01, loss_function=RMSE, subsample=0.9; total time=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "112 fits failed out of a total of 210.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "84 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 5807, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 2381, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 2307, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6382, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6404, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: /Users/zomb-ml-platform-msk/go-agent-21.2.0/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/bootstrap_options.cpp:16: Error: bayesian bootstrap doesn't support 'subsample' option\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 5807, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 2396, in _fit\n",
      "    self._train(\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 1776, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4833, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4882, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: /Users/zomb-ml-platform-msk/go-agent-21.2.0/pipelines/BuildMaster/catboost.git/catboost/libs/metrics/metric.cpp:4939: Metric Huber requires delta as parameter\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.4715768  0.47015779        nan 0.47574475 0.47325264 0.46480892\n",
      "        nan        nan 0.4789742  0.47437554        nan 0.47538271\n",
      " 0.47314264 0.47342174        nan        nan        nan        nan\n",
      "        nan        nan 0.47024165        nan        nan        nan\n",
      "        nan 0.48015043        nan 0.47965447 0.47618431        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Optimized CatBoost Hyperparameters:\n",
      " {'subsample': 0.8, 'loss_function': 'RMSE', 'learning_rate': 0.02, 'l2_leaf_reg': 3, 'iterations': 700, 'depth': 5, 'colsample_bylevel': 1.0, 'bootstrap_type': 'Bernoulli'}\n",
      " Final Optimized R Score (CatBoost): 0.4641\n",
      " Final Optimized MSE (CatBoost): 87.3564\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "selected_features = selected_features = ['depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4',\n",
    "       'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10',\n",
    "       'cut_Good', 'color_E', 'color_G', 'color_H', 'color_I', 'color_J', \n",
    "       'clarity_SI1', 'clarity_VS1', 'clarity_VVS2', 'volume', 'log_price', 'log_carat']\n",
    "\n",
    "df_selected_features = df_transformed[['outcome'] + selected_features]\n",
    "\n",
    "#  Step 2: Define Features and Target\n",
    "target_col = \"outcome\"\n",
    "X = df_selected_features.drop(columns=[target_col])\n",
    "y = df_selected_features[target_col]\n",
    "\n",
    "#  Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#  Define Hyperparameter Grid for Tuning\n",
    "cat_param_dist = {\n",
    "    \"iterations\": [500, 700, 1000],  # More boosting rounds\n",
    "    \"learning_rate\": [0.01, 0.02, 0.03],  # Slower for stability\n",
    "    \"depth\": [4, 5, 6, 7],  # Test deeper trees\n",
    "    \"l2_leaf_reg\": [1, 3, 5, 7],  # Regularization tuning\n",
    "    \"loss_function\": ['RMSE', 'Huber', 'Quantile'],  # Robust loss functions\n",
    "    \"colsample_bylevel\": [0.8, 0.9, 1.0],  # Feature selection per level\n",
    "    \"subsample\": [0.7, 0.8, 0.9],  # Sample diversity control\n",
    "    \"bootstrap_type\": ['Bayesian', 'Bernoulli'],  # Better handling of bootstrapping\n",
    "}\n",
    "\n",
    "#  Initialize CatBoost Model (No Verbose for Faster Training)\n",
    "cat_model = CatBoostRegressor(random_seed=42, verbose=0)\n",
    "\n",
    "#  Perform Hyperparameter Tuning\n",
    "cat_random_search = RandomizedSearchCV(\n",
    "    cat_model,\n",
    "    param_distributions=cat_param_dist,\n",
    "    n_iter=30,  # More iterations for better tuning\n",
    "    cv=7,  # More folds for stability\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "#  Train Model with Best Parameters\n",
    "cat_random_search.fit(X_train, y_train)\n",
    "\n",
    "#  Get Best Model\n",
    "best_cat_params = cat_random_search.best_params_\n",
    "final_cat_model = cat_random_search.best_estimator_\n",
    "\n",
    "#  Evaluate on Validation Set\n",
    "y_pred_cat = final_cat_model.predict(X_val)\n",
    "final_r2_cat = r2_score(y_val, y_pred_cat)\n",
    "final_mse_cat = mean_squared_error(y_val, y_pred_cat)\n",
    "\n",
    "#  Print Results\n",
    "print(\"\\n Final Optimized CatBoost Hyperparameters:\\n\", best_cat_params)\n",
    "print(f\" Final Optimized R Score (CatBoost): {final_r2_cat:.4f}\")\n",
    "print(f\" Final Optimized MSE (CatBoost): {final_mse_cat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80de66b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      " Best Random Forest Parameters: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_depth': 10}\n",
      " Best Random Forest Cross-Validation R: 0.45739808218218825\n",
      "\n",
      " CatBoost Cross-Validation R Scores: [0.4782336  0.4824774  0.48955471 0.48482272 0.45656308]\n",
      " Mean CatBoost Cross-Validation R: 0.47833030155462736\n",
      "\n",
      " CatBoost is still the best model! Stick with CatBoost for submission.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Load the training dataset\n",
    "train_file_path = \"CW1_train.csv\"\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "\n",
    "# Define the selected features\n",
    "selected_features = ['depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4',\n",
    "                     'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10',\n",
    "                     'cut_Good', 'color_E', 'color_G', 'color_H', 'color_I', 'color_J', \n",
    "                     'clarity_SI1', 'clarity_VS1', 'clarity_VVS2', 'volume', 'log_price', 'log_carat']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Create 'volume' feature\n",
    "train_data['volume'] = train_data['x'] * train_data['y'] * train_data['z']\n",
    "\n",
    "# Apply log transformation\n",
    "train_data['log_price'] = np.log1p(train_data['price'])\n",
    "train_data['log_carat'] = np.log1p(train_data['carat'])\n",
    "\n",
    "# Drop original features that were transformed\n",
    "train_data.drop(columns=['x', 'y', 'z', 'price', 'carat'], inplace=True)\n",
    "\n",
    "# Extract training features and target\n",
    "X_trn = train_data[selected_features]\n",
    "y_trn = train_data['outcome']\n",
    "\n",
    "# Define hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500],  # Number of trees\n",
    "    'max_depth': [5, 10, 20, None],  # Maximum depth of trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples to split a node\n",
    "    'min_samples_leaf': [1, 2, 5]  # Minimum samples in a leaf node\n",
    "}\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestRegressor(n_jobs=-1, random_state=123)\n",
    "\n",
    "# Perform Randomized Search with Cross-Validation (5-fold)\n",
    "rf_search = RandomizedSearchCV(rf_model, param_distributions=param_grid, \n",
    "                               n_iter=20, scoring='r2', cv=5, verbose=1, random_state=123)\n",
    "\n",
    "# Fit model\n",
    "rf_search.fit(X_trn, y_trn)\n",
    "\n",
    "# Get best parameters and score for Random Forest\n",
    "best_rf_params = rf_search.best_params_\n",
    "best_rf_r2 = rf_search.best_score_\n",
    "\n",
    "print(\"\\n Best Random Forest Parameters:\", best_rf_params)\n",
    "print(\" Best Random Forest Cross-Validation R:\", best_rf_r2)\n",
    "\n",
    "# Now test CatBoost\n",
    "catboost_model = CatBoostRegressor(\n",
    "    subsample=0.8,\n",
    "    loss_function='RMSE',\n",
    "    learning_rate=0.02,\n",
    "    l2_leaf_reg=3,\n",
    "    iterations=700,\n",
    "    depth=5,\n",
    "    colsample_bylevel=1.0,\n",
    "    bootstrap_type='Bernoulli',\n",
    "    verbose=0  # Suppress training logs\n",
    ")\n",
    "\n",
    "# Perform cross-validation for CatBoost\n",
    "cv_r2_scores_catboost = cross_val_score(catboost_model, X_trn, y_trn, scoring='r2', cv=5)\n",
    "mean_cv_r2_catboost = cv_r2_scores_catboost.mean()\n",
    "\n",
    "print(\"\\n CatBoost Cross-Validation R Scores:\", cv_r2_scores_catboost)\n",
    "print(\" Mean CatBoost Cross-Validation R:\", mean_cv_r2_catboost)\n",
    "\n",
    "# Final comparison\n",
    "if best_rf_r2 > mean_cv_r2_catboost:\n",
    "    print(\"\\n Random Forest performed better! Use the tuned RF model.\")\n",
    "else:\n",
    "    print(\"\\n CatBoost is still the best model! Stick with CatBoost for submission.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38fcf01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "35 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 5807, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 2381, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/catboost/core.py\", line 2307, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6382, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6404, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: /Users/zomb-ml-platform-msk/go-agent-21.2.0/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/bootstrap_options.cpp:16: Error: bayesian bootstrap doesn't support 'subsample' option\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.47819231 0.47851747 0.46976723 0.47509116 0.47806134 0.47545043\n",
      " 0.47569098        nan        nan        nan 0.47367989        nan\n",
      "        nan 0.47582189 0.47580148 0.47734109        nan 0.47119944\n",
      "        nan 0.46045814]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best CatBoost Parameters: {'subsample': 0.7, 'learning_rate': 0.05, 'l2_leaf_reg': 9, 'iterations': 500, 'depth': 4, 'colsample_bylevel': 0.8, 'bootstrap_type': 'Bernoulli'}\n",
      " Best CatBoost Cross-Validation R: 0.47851746703957565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('CW1_train.csv')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Create 'volume' feature\n",
    "train_data['volume'] = train_data['x'] * train_data['y'] * train_data['z']\n",
    "\n",
    "# Apply log transformation\n",
    "train_data['log_price'] = np.log1p(train_data['price'])\n",
    "train_data['log_carat'] = np.log1p(train_data['carat'])\n",
    "\n",
    "# Drop original features that were transformed\n",
    "train_data.drop(columns=['x', 'y', 'z', 'price', 'carat'], inplace=True)\n",
    "\n",
    "# Define selected features\n",
    "selected_features = ['depth', 'table', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4',\n",
    "                     'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10',\n",
    "                     'cut_Good', 'color_E', 'color_G', 'color_H', 'color_I', 'color_J', \n",
    "                     'clarity_SI1', 'clarity_VS1', 'clarity_VVS2', 'volume', 'log_price', 'log_carat']\n",
    "\n",
    "# Extract features & target\n",
    "X_trn = train_data[selected_features]\n",
    "y_trn = train_data['outcome']\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'iterations': [500, 700, 1000], \n",
    "    'learning_rate': [0.01, 0.02, 0.05],  \n",
    "    'depth': [4, 5, 6, 7],  \n",
    "    'l2_leaf_reg': [2, 3, 5, 7,9],  \n",
    "    'colsample_bylevel': [0.8, 1.0],  \n",
    "    'subsample': [0.7, 0.8, 0.9],  \n",
    "    'bootstrap_type': ['Bayesian', 'Bernoulli']\n",
    "}\n",
    "\n",
    "# Initialize CatBoost model\n",
    "catboost_model = CatBoostRegressor(loss_function='RMSE', verbose=0, random_state=123)\n",
    "\n",
    "# Perform Randomized Search with Cross-Validation (5-fold)\n",
    "catboost_search = RandomizedSearchCV(catboost_model, param_distributions=param_grid, \n",
    "                                     n_iter=20, scoring='r2', cv=5, verbose=1, random_state=123)\n",
    "\n",
    "# Fit model to find best parameters\n",
    "catboost_search.fit(X_trn, y_trn)\n",
    "\n",
    "# Get best parameters and score\n",
    "best_catboost_params = catboost_search.best_params_\n",
    "best_catboost_r2 = catboost_search.best_score_\n",
    "\n",
    "print(\"\\n Best CatBoost Parameters:\", best_catboost_params)\n",
    "print(\" Best CatBoost Cross-Validation R:\", best_catboost_r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
